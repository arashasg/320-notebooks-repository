{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint('directory contents:', ', '.join(os.listdir('/kaggle/input/deepfake-detection-challenge')))\n\nprint(\n    'num train videos:', len(os.listdir('/kaggle/input/deepfake-detection-challenge/train_sample_videos/')) - 1,\n    '\\nnum test videos: ',  len(os.listdir('/kaggle/input/deepfake-detection-challenge/test_videos/'))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 as cv\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir) if x.endswith('.mp4')]\ntest_dir = '/kaggle/input/deepfake-detection-challenge/test_videos/'\ntest_video_files = [test_dir + x for x in os.listdir(test_dir)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Update:**<br>\nMy mistake, I thought we didn't have access to the train_sample labels, but we do and they are there hiding as a json file in the train_sample videos folder.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_metadata = pd.read_json('/kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json')\ntrain_metadata = train_metadata.T\ntrain_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_metadata['label'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are about 80% of the labels FAKE for train and test?  Would be interested to know for the full training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_first_frame(video_files, num_to_show=25):\n    root = int(num_to_show**.5)\n    fig, axes = plt.subplots(root,root, figsize=(root*5,root*5))\n    for i, video_file in tqdm(enumerate(video_files[:num_to_show]), total=num_to_show):\n        cap = cv.VideoCapture(video_file)\n        success, image = cap.read()\n        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n        cap.release()   \n        \n        axes[i//root, i%root].imshow(image)\n        fname = video_file.split('/')[-1]        \n        try:\n            label = train_metadata.loc[fname, 'label']\n            axes[i//root, i%root].title.set_text(f\"{fname}: {label}\")\n        except:\n            axes[i//root, i%root].title.set_text(f\"{fname}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_first_frame(train_video_files, num_to_show=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test videos"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_first_frame(test_video_files, num_to_show=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There appears to be multiple videos per person in the train_sample videos and test videos."},{"metadata":{},"cell_type":"markdown","source":"## This test video frame looks very fake"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,12))\ncap = cv.VideoCapture(test_dir + 'ahjnxtiamx.mp4')\ncap.set(1,2)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \n\nax.imshow(image)\nfname = 'ahjnxtiamx.mp4'\nax.title.set_text(f\"{fname}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is just one example, but a few questions it makes me think of are: <br><br>\n1) Were many different GANs architectures used to generate the dataset? (probably yes since many different teams collaberated to create the full dataset).   <br><br>\n2) Does it change our modelling approach if there's some very obvious fakes (created from older methods) in the dataset, mixed in with the very realistic fakes (created from newer methods)? "},{"metadata":{},"cell_type":"markdown","source":"## Wheres the full training data?\n\n*Copied from the getting-started page . https://www.kaggle.com/c/deepfake-detection-challenge/overview/getting-started*\n\n<div><div class=\"markdown-converter__text--rendered\"><h3><strong>Datasets</strong>:</h3>\n<p>There are 4 groups of datasets associated with this competition.</p>\n<ol>\n<li><strong>Training Set: This dataset, containing labels for the target, is available for download outside of Kaggle for competitors to build their models.</strong> It is broken up into 50 files, for ease of access and download. Due to its large size, it must be accessed through a GCS bucket which is only made available to participants after accepting the competition’s rules. Please read the rules fully before accessing the dataset, as they contain important details about the dataset’s permitted use. It is expected and encouraged that you train your models outside of Kaggle’s notebooks environment and submit to Kaggle by uploading the trained model as an external data source.</li>\n<li><strong>Public Validation Set</strong>: When you commit your Kaggle notebook, the submission file output that is generated will be based on the small set of 400 videos/ids contained within this Public Validation Set. This is available on the Kaggle Data page as <code>test_videos.zip</code></li>\n<li><strong>Public Test Set: This dataset is completely withheld and is what Kaggle’s platform computes the public leaderboard against.</strong> When you “Submit to Competition” from the “Output” file of a committed notebook that contains the competition’s dataset, your code will be re-run in the background against this Public Test Set. When the re-run is complete, the score will be posted to the public leaderboard. If the re-run fails, you will see an error reflected in your “My Submissions” page. Unfortunately, we are unable to surface any details about your error, so as to prevent error-probing. You are limited to 2 submissions per day, including submissions which error.</li>\n<li><strong>Private Test Set: This dataset is privately held outside of Kaggle’s platform, and is used to compute the private leaderboard.</strong> It contains videos with a similar format and nature as the Training and Public Validation/Test Sets, but are real, organic videos with and without deepfakes. After the competition deadline, Kaggle transfers your 2 final selected submissions’ code to the host. They will re-run your code against this private dataset and return prediction submissions back to Kaggle for computing your final private leaderboard scores.</li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}