{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"#### Introduction\nIn this kernal I will focus on keyphrase extraction from NLP research papers using graph-based algorithms implemented in pke package. Later will also do some experiments using these keyphrases."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Installing pke\n\n!pip install git+https://github.com/boudinfl/pke.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AffinityPropagation\nimport re\nimport pke","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"papers = pd.read_csv('../input/201812_CL_Github.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.shape\n#Total 106 papers given","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keyphrase Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keyphrase extraction(top 10) from abstracts using textrank algorithm\n\ndef extract_keyphrases(caption, n):\n    extractor = pke.unsupervised.TextRank() \n    extractor.load_document(caption)\n    extractor.candidate_selection()\n    extractor.candidate_weighting()\n    keyphrases = extractor.get_n_best(n=n, stemming=False)\n    print(keyphrases,\"\\n\")\n    return(keyphrases)\n    \npapers['Abstract_Keyphrases'] = papers.apply(lambda row: (extract_keyphrases(row['Abstract'],10)),axis=1)","execution_count":24,"outputs":[{"output_type":"stream","text":"[('discourse dependency structures', 0.12811645856795542), ('multi - party dialogues', 0.10025102656641602), ('various nlp tasks such', 0.10025068656641602), ('dependency relations', 0.08458330261772845), ('deep sequential model', 0.08274360188426316), ('discourse structures', 0.08199330792172), ('elementary discourse units', 0.0742503515007769), ('edu sequence', 0.06523762720211033), ('concerned edus', 0.06523751720211032), ('previous edu', 0.06523695720211033)] \n\n[('useful speaker representations', 0.06972180037381355), ('learning good representations', 0.06086258764306304), ('different objective functions', 0.05430091253393665), ('- supervised settings', 0.054300812533936656), ('raw speech waveform', 0.05430000253393665), ('generative adversarial networks', 0.054299482533936655), ('high dimensional spaces', 0.05429918253393665), ('speaker identification', 0.05007968313152715), ('speaker identities', 0.05007866313152715), ('architecture similar', 0.04699610630452334)] \n\n[('current fake news research', 0.15535480924421202), ('potential research tasks', 0.09438440772520502), ('fake news analysis', 0.09407412273910819), ('specifies fundamental theories', 0.08368248836763804), ('interdisciplinary research', 0.07745216233706537), ('fake news', 0.07706913583403768), ('various strategies', 0.07316201957315516), ('various analyzable', 0.07316188957315516), ('various disciplines', 0.07316101957315516), ('research', 0.060520836948925724)] \n\n[('training dialogue reward predictors', 0.1787294478486579), ('dialogue reward prediction', 0.16088756509985838), ('target dialogue rewards', 0.15936125897376496), ('lengthy dialogue histories', 0.15280932977326844), ('dialogue rewards', 0.14151920622496547), ('human dialogues', 0.12531959898954248), ('dialogue history', 0.1217809929990048), ('optimal context windows', 0.09966811408246844), ('system performance', 0.08056587404859412), ('conversational system', 0.08056542404859413)] \n\n[('cross - domain performance', 0.09234450374810088), ('exact lexical matching tasks', 0.08382125992199933), ('cross - topic', 0.07341725511762925), ('cross - temporal', 0.06308797428980602), ('controversy detection methods', 0.05808463266214908), ('weak signal data', 0.05808444266214908), ('filter problematic content', 0.05808348266214908), ('general web pages', 0.05808329266214908), ('model performance', 0.05251444222424549), ('semantic properties', 0.050786812533334884)] \n\n[('distant user information', 0.0709560415423658), ('stable user representation', 0.06938340958248815), ('user features', 0.06781042864008126), ('generalized canonical correlation analysis', 0.0660084606600616), ('mental health state prediction', 0.06422420142478012), ('user behavior', 0.06159794415204364), ('many downstream tasks', 0.06091293118582041), ('user representations', 0.06058485339145845), ('mental health features', 0.055291210312987345), ('ground truth gender', 0.04950758049504618)] \n\n[('supervised learning pipeline', 0.09426400932004114), ('transfer learning technique', 0.08539092977076856), ('supervised learning', 0.08261944769443612), ('learning approaches', 0.07374742814516355), ('machine learning', 0.07374573814516355), ('external source hospital', 0.06590176330369879), ('supervised cnn model', 0.06336558555700002), ('learning', 0.06210272651955853), ('radiology report classification', 0.061854228969646076), ('specific feature engineering', 0.05994211993953291)] \n\n[('general pre - trained word embeddings', 0.15604906128582754), ('7th dialogue system technology challenges', 0.10080664161232955), ('end response selection model', 0.09506377987897136), ('new pooling method', 0.08156231308721901), ('original esim model', 0.06613337263097299), ('sequential inference model', 0.066133202630973), ('multi - dimensional', 0.06410034240093415), ('specific training set', 0.062349827599117114), ('hierarchical recurrent encoder', 0.06048529096739773), ('correct next utterance', 0.06048422096739773)] \n\n[('accurate neural dialogue state tracking model', 0.18602954673737332), ('dialogue state tracking models', 0.1189235505798013), ('belief state tracking', 0.07938560673726784), ('local modules', 0.07441228615358299), ('joint goal accuracy', 0.07398477736126982), ('\\\\ # slots', 0.07398392736126982), ('dialogue states', 0.06388754810062118), ('local conditioning', 0.060385794661295566), ('local self', 0.06038505466129557), ('turn inform', 0.055094288884158474)] \n\n[('semi - structured knowledge bases', 0.15037609984960112), ('formal query languages', 0.11077979620493068), ('semantic parsing system', 0.09929804514569589), ('semantic parsing deals', 0.09929769514569589), ('natural language utterances', 0.09320708353569729), ('current neural approaches', 0.09022661390976068), ('key challenges', 0.06015165593984043), ('initial rule', 0.06015136593984043), ('prominent work', 0.06015130593984044), ('various components', 0.060151205939840434)] \n\n[('stanford natural language inference dataset', 0.20322595213215822), ('annotated natural language explanations', 0.16577976347593165), ('natural language explanations', 0.14514717778431618), ('universal sentence representations', 0.1251158907264582), ('full sentence justifications', 0.1251157807264582), ('widespread public adoption', 0.10152292263934241), ('train time', 0.08786810347558896), ('research directions', 0.06768342509289492), ('test time', 0.06768278509289495), ('training process', 0.06768272509289494)] \n\n[('real world text classification', 0.08274431202923631), ('scale unsupervised language modeling', 0.07464219663900487), ('world sentiment classification', 0.06693300935431412), ('emotion sentiment classification', 0.05997657383417579), ('mohammad et al', 0.0534299397496122), ('mcauley et al', 0.0534295597496122), ('vaswani et al', 0.0534293897496122), ('emotion classification problem', 0.04943015947374074), ('natural language processing', 0.04939616970350483), ('unsupervised language', 0.04843386058166986)] \n\n[('fluent casual conversation', 0.19612404018624893), ('tartan conversational agent', 0.19466006209445424), ('other conversational agent', 0.17490501718292764), ('structured conversation', 0.17298278677830128), ('engaging conversations', 0.1532268618667747), ('conversational acts', 0.1532268118667747), ('finite state machine', 0.12372948671473143), ('non - goal', 0.11235975056057693), ('state models', 0.09764463934869369), ('dialog manager', 0.07490736704038461)] \n\n[('communicative bias functions', 0.12074509023594475), ('language communication systems', 0.10658040107423486), ('arbitrary phase transition point', 0.10217239656817788), ('natural language processing applications', 0.10008236176057869), ('image classification task', 0.07663024242613345), ('novel integral transform', 0.07662940242613345), ('zipfian word', 0.07422381646168363), ('word rank', 0.07422164646168364), ('listener bias', 0.06910136602618754), ('theoretic bias', 0.06910057602618754)] \n\n[('neural sequence models', 0.09588259444178886), ('wrong annotations', 0.06768972182201533), ('right annotations', 0.06768955182201533), ('level annotations', 0.06768938182201534), ('lstm model', 0.0635250760642586), ('other models', 0.06352376606425861), ('classification accuracy', 0.05793634401657), ('sentiment classification', 0.057935064016570004), ('most cases', 0.05208519333330278), ('further analysis', 0.05208514333330278)] \n\n[('toxicity detection tool', 0.15189410121117256), ('toxicity detection', 0.1280948865544205), ('refined comment filters', 0.11560722641250203), ('sentiment detection', 0.10812517813058682), ('toxic users', 0.08900339317464644), ('toxic content', 0.08900309317464644), ('positive impact', 0.07707274094166802), ('world datasets', 0.07707246094166802), ('different real', 0.07707243094166802), ('various aspects', 0.07707205094166802)] \n\n[('other semantic similarity methods', 0.14166687704625308), ('novel semantic similarity method', 0.1416660670462531), ('multi - dimensional medicine', 0.14114144644015159), ('drug abuse prevention', 0.10111138705087613), ('specific feature vectors', 0.09787989221859705), ('medical knowledge graphs', 0.09787947221859705), ('drug substitution', 0.07912009646003261), ('case study', 0.06525396481239805), ('promising applications', 0.06525381481239804), ('significant improvement', 0.06525369481239805)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('directional recurrent networks such', 0.17094058196266732), ('single backward pass', 0.14095169201206734), ('art methods', 0.11096154206146733), ('new state', 0.09566736303231382), ('current state', 0.09566655303231383), ('art results', 0.08547125598133366), ('single feed', 0.08547105598133366), ('entire sequence', 0.08547097598133367), ('video summarization', 0.08547092598133367), ('attention mechanism', 0.08547037598133367)] \n\n[('sophisticated natural language processing', 0.08634600528969595), ('several language information sources', 0.07929096290687919), ('user activity logs', 0.06537111610187978), ('time capable named entity', 0.06334288098961187), ('knowledge work support systems', 0.06334152098961186), ('explicit user input', 0.05606995042305281), ('german language', 0.05428396971510245), ('high speed methods', 0.052231381983551366), ('information extraction task', 0.0512422307006888), ('much runtime performance', 0.0475087382422089)] \n\n[('mean reciprocal rank', 0.12588833010767325), ('answer selection task', 0.12588817010767325), ('span prediction models', 0.12588738010767325), ('long summaries', 0.08545862302775721), ('long documents', 0.08545826302775722), ('task baseline', 0.07633690786256118), ('comprehension dataset', 0.07633675786256118), ('model performance', 0.07633663786256119), ('global normalization', 0.07633635786256118), ('answer selection', 0.07633628786256119)] \n\n[('several different neural network components', 0.11740539680102155), ('neural abstractive text summarization', 0.09366196529460666), ('summary generation algorithms', 0.07804162092168389), ('daily mail dataset', 0.06376432535350154), ('open source library', 0.06376396535350154), ('past few years', 0.06376198535350154), ('abstractive text summarization', 0.06094093448746549), ('language modeling', 0.055817987185376956), ('many models', 0.05581791718537696), ('seq2seq models', 0.05581680718537696)] \n\n[('different sa competitions', 0.1587052358095704), ('several sa competitions', 0.14448619737163035), ('various sa competitions', 0.14448528737163033), ('competitive sa', 0.12640827587824033), ('different text classifiers', 0.1170094752934471), ('competitive obtaining top', 0.10386326836274294), ('sa system', 0.09549273452097659), ('text models', 0.08471193536211702), ('processing text', 0.08471160536211703), ('sa', 0.07741354302758657)] \n\n[('multi - view attention scheme', 0.15558861065075216), ('novel multi - task', 0.14461602047805835), ('multi - task learning', 0.14403457454546825), ('multi - view attention', 0.13974824461686253), ('comprehensive sentence representations', 0.09742270507179493), ('different representational perspectives', 0.09675678484405309), ('rich correlation information', 0.0912314314907621), ('knowledge base question', 0.09098612161563932), ('attentive information', 0.0844592948341841), ('kbqa tasks', 0.07314821469247546)] \n\n[('speech recognition output style text', 0.2035788794578141), ('conventional pipeline system', 0.1513004196435239), ('speech translation task', 0.14853771775883792), ('translation models', 0.1281205956679401), ('machine translation', 0.12812050566794012), ('post - processing', 0.1232036554209445), ('baseline system', 0.10027071660502196), ('nel system', 0.10026982660502197), ('speech recognition', 0.08500910010123536), ('translation', 0.08269865239563984)] \n\n[('various deep learning architectures', 0.12037330571424283), ('architectural design choices', 0.09329545703089795), ('standard lstm model', 0.08371494565906419), ('natural language questions', 0.07918962041404971), ('standard softmax classifier', 0.07203077152460981), ('architectures shows', 0.06511232319689292), ('different architectures', 0.06511037319689292), ('better results', 0.0533940310877251), ('same strategy', 0.053392531087725105), ('better understanding', 0.053391901087725104)] \n\n[('emr data analysis', 0.08370093873014466), ('text electronic medical records', 0.08309090440367234), ('free emr texts', 0.08302157273887413), ('emr data', 0.07607594482416799), ('medical data', 0.06901608396284166), ('data augmentation method', 0.06858358741823439), ('such massive data', 0.06582981268074393), ('medical text', 0.06260688914882907), ('diverse emr samples', 0.053631623001233766), ('possible new discoveries', 0.050166889406385126)] \n\n[('neural network topology', 0.20049000155238553), ('previous approaches', 0.13587934614736702), ('previous activations', 0.135878946147367), ('better use', 0.12157436090911825), ('main contributions', 0.12157416090911825), ('deep neural', 0.12157385090911826), ('feature generation', 0.12157376090911824), ('network', 0.07891600064326727), ('contribution', 0.060787875454559125), ('size', 0.00912198691698831)] \n\n[('senator bernie sanders', 0.09788022221859581), ('stable political movement', 0.0978796322185958), ('critical success factor', 0.09787932221859581), ('further investigation', 0.06525426481239721), ('collected data', 0.06525418481239721), ('informatics experts', 0.06525406481239722), ('economic reasons', 0.06525388481239722), ('us politician', 0.06525376481239722), ('large number', 0.06525358481239722), ('public opinions', 0.06525352481239721)] \n\n[('adversarial dialogue generation models', 0.15120306783276444), ('new reward model', 0.10073799248661026), ('dialogue generation method', 0.09670452933109475), ('adversarial inverse reinforcement', 0.08991533472565985), ('precise reward signal', 0.0893897440726192), ('dialogue generation', 0.08130785792115225), ('higher overall performance', 0.08097320991902833), ('reward signal', 0.07331491019078414), ('generator training', 0.06937945365162505), ('adversarial imitation', 0.05845177488126465)] \n\n[('latest bert contextual model', 0.13086265262590782), ('traditional mrc models', 0.10081154342653302), ('novel qa task', 0.08926248856527029), ('previous best model', 0.08777555110920508), ('inter - attention', 0.08119159837616982), ('deep neural network', 0.08119140837616982), ('conversation context', 0.07027129929564746), ('art result', 0.06051757261713261), ('empirical results', 0.06051740261713261), ('contextual understanding', 0.05833283211222585)] \n\n[('asynchronous data collection', 0.1222004018839969), ('conversational roles', 0.0987783132508595), ('conversational interactions', 0.0987781932508595), ('geometric reasoning', 0.08146762458933128), ('dialog tasks', 0.08146755458933128), ('multimodal goal', 0.08146751458933127), ('comprehensive quality', 0.08146727458933127), ('editable canvas', 0.08146690458933127), ('multiple agents', 0.08146666458933127), ('layout composition', 0.08146655458933127)] \n\n[('universal word embeddings', 0.31453485539307446), ('good word representation', 0.24968942441566205), ('embedding learning method', 0.21956862521340903), ('delta embedding learning', 0.21748693152451015), ('unsupervised word', 0.21078714954139918), ('supervised nlp tasks', 0.19543980941367992), ('nlp tasks', 0.14532445476350184), ('simple technique', 0.13029378960911991), ('learning', 0.07448277680939219), ('tasks', 0.050116494650178076)] \n\n[('many empirical observations', 0.10309366350499831), ('pairwise inner product', 0.10309310350499831), ('optimal dimensionality', 0.07676165538741052), ('dimensionality selection', 0.07676133538741052), ('open question', 0.06873004233666556), ('pip loss', 0.06872995233666555), ('new insights', 0.06872960233666556), ('variance trade', 0.06872917233666556), ('fundamental bias', 0.06872914233666556), ('perturbation theory', 0.06872908233666555)] \n\n[('relation alias information', 0.13041084916239365), ('neural relation extraction method', 0.12510036654338927), ('additional side information', 0.12089459483442747), ('available side information', 0.12089435483442745), ('relevant side information', 0.12089402483442746), ('relation extraction', 0.09405796149068318), ('limited side', 0.0820287079752133), ('relation founderofcompany', 0.08048655770380492), ('relation instances', 0.08048608770380491), ('graph convolution networks', 0.0776209536481237)] \n\n[('training anomaly detectors', 0.08253882886210505), ('deep anomaly', 0.07973282169536972), ('edge generative models', 0.07220354606498178), ('scale vision tasks', 0.07220332606498178), ('natural language processing', 0.07220324606498178), ('deep learning magnifies', 0.07100936306639327), ('unseen anomalies', 0.06761465997231249), ('cifar-10 images', 0.06271051121358893), ('svhn images', 0.06271047121358894), ('diverse image', 0.06270943121358893)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('pre - trained word embeddings', 0.1143873901809849), ('softmax layer', 0.06417792048361605), ('novel probabilistic loss', 0.06389857357827479), ('large memory footprint', 0.06389830357827478), ('reference translations', 0.055407344854719376), ('translation quality', 0.05540685485471938), ('machine translation', 0.05540649485471938), ('slowest layer', 0.051972096707184244), ('final layer', 0.051971856707184245), ('training time', 0.0494610521389667)] \n\n[('neural machine translation tasks', 0.11722045008676071), ('non - smooth prediction', 0.11005002857973448), ('multi - class classification', 0.11004995857973447), ('english translation task', 0.10212226878506919), ('text summarization task show', 0.09472568991947442), ('non - smoothness', 0.09467991454713402), ('wise regularization method', 0.07919062041404971), ('promising bleu scores', 0.07203063152460983), ('conventional mle loss', 0.07203053152460982), ('target token', 0.062341035461953014)] \n\n[('concordance correlation coefficient', 0.1025652825640641), ('multimodal lstm model', 0.10256484256406409), ('empathy prediction challenge', 0.10256466256406409), ('text features', 0.07632714135319989), ('visual features', 0.07632698135319989), ('empathic responses', 0.07632689135319988), ('empathic listener', 0.07632632135319989), ('test set', 0.06837762837604272), ('local attention', 0.06837690837604272), ('level fusion', 0.06837687837604273)] \n\n[('word level nmt model', 0.14234893366234563), ('neural machine translation', 0.09451215907849983), ('statistical machine translation', 0.09451208907849983), ('core models viz', 0.08728791509515954), ('word level nmt', 0.08628447233022109), ('bengali language pairs', 0.07722126721832363), ('smt model', 0.07167637821364205), ('models', 0.056064931332124535), ('manual parameters', 0.05148208147888242), ('ter metrics', 0.05148204147888242)] \n\n[('multi - scale structural similarity index', 0.14117794058823527), ('negative training examples', 0.10567685445213285), ('random training examples', 0.09279837274738355), ('oxford-102 flower dataset', 0.07058960529411767), ('training methods', 0.06547933089453148), ('semantic space', 0.052360765057868686), ('greater diversity', 0.052360515057868685), ('semantic distance', 0.05235996505786868), ('class label', 0.05235973505786869), ('certain classes', 0.05235936505786869)] \n\n[('multi - speaker neural tts model', 0.24278829628395301), ('multi - speaker latent space', 0.20801947857926462), ('speaker similarity close', 0.16201207281447477), ('single speaker result', 0.16029717168307342), ('new speakers', 0.14257290843659085), ('speaker information', 0.13108098214142144), ('other speakers', 0.13108074214142143), ('speaker', 0.11600320704968492), ('domain texts', 0.05657867628005477), ('human recordings', 0.05657838628005477)] \n\n[('language documentation process', 0.14068364769969932), ('natural language processing', 0.1272833020404075), ('new languages', 0.09774919893986853), ('language documentation', 0.09381749511965738), ('natural language', 0.08041858946036559), ('phoneme transcription', 0.0666142500951949), ('current progress', 0.0666141700951949), ('multilingual neural', 0.06661391009519489), ('recent advances', 0.0666138700951949), ('nlp tools', 0.06661378009519489)] \n\n[('dynamic feature generation network', 0.15000303120816152), ('answer selection datasets', 0.09508837323270553), ('feature augmentation', 0.09152345188492207), ('appropriate features', 0.09152312188492208), ('feature', 0.07143743425629732), ('attention mechanisms', 0.07057151577450124), ('sentence level', 0.07057117577450124), ('lexical level', 0.07057104577450124), ('previous attention', 0.07057094577450124), ('interpretative ability', 0.06339303215513702)] \n\n[('cross - task features', 0.1654952773102728), ('natural language understanding', 0.11551087755038204), ('various semantic information', 0.11512280096363273), ('story cloze test', 0.11512267096363271), ('nli task', 0.10125203105843175), ('task performance', 0.10125100105843175), ('language inference', 0.09140625925434183), ('tasks', 0.07928621282875145), ('previous work', 0.07790787717238515), ('previous methods', 0.07790732717238515)] \n\n[('end acoustic system', 0.1967783923153096), ('better quality audios', 0.13545255525092095), ('parallel acoustic system', 0.13366734261938504), ('non - autoregressive', 0.12320379542094449), ('end text', 0.1123518750624971), ('popular end', 0.11235182506249711), ('lightweight end', 0.1123510650624971), ('acoustic system', 0.10924659598802287), ('speech systems', 0.09221705347084558), ('convolutional structure', 0.09145734219749034)] \n\n[('multi - level taxonomy', 0.1442283441450909), ('e - commerce platforms', 0.10422277522549799), ('product taxonomy', 0.09290864401079676), ('meaningful new paths', 0.0865330357222616), ('resultant taxonomy dag', 0.08470109149968813), ('new products', 0.08259829426065224), ('machine translation models', 0.0719439860431651), ('art classification system', 0.07194383604316511), ('natural language description', 0.0719433160431651), ('taxonomy tree', 0.07121003620650611)] \n\n[('novel convolutional neural network', 0.09692220814259608), ('raw audio samples', 0.07327073332947878), ('deep neural networks', 0.0718042228445011), ('neural architecture', 0.07040222005037888), ('clear physical meaning', 0.06342697714567762), ('high cutoff frequencies', 0.06342657714567762), ('neural networks', 0.0588279728948894), ('speech recognition', 0.05489626610103939), ('audio waveforms', 0.05126957894268448), ('raw waveform', 0.05126907894268448)] \n\n[('multi - task learning', 0.11375562463876768), ('available medical literature datasets', 0.10471360188481678), ('multi - task', 0.09661995692151443), ('model recognition', 0.07898549037078106), ('explicit feedback strategies', 0.0785346514136126), ('novel deep neural', 0.0785345514136126), ('mutual benefits', 0.06797128458455812), ('model performance', 0.06409214766936615), ('hierarchical tasks', 0.06409189766936615), ('joint modelling', 0.06409098766936615)] \n\n[('- supervised learning techniques', 0.18792577146077022), ('shot learning method', 0.14841826283918247), ('many natural language processing', 0.13422820791946233), ('metric learning', 0.12828507674464384), ('transfer learning', 0.12828482674464384), ('learning', 0.10815216065010523), ('entity classes', 0.08713133814960622), ('target class', 0.06711541395973117), ('training examples', 0.06711502395973117), ('limited number', 0.06711499395973117)] \n\n[('inter - sentential relation extraction task', 0.32038761032423435), ('non - trivial', 0.15220292970298424), ('event relations', 0.14316852460336887), ('cell type', 0.10245372353677719), ('tissue type', 0.10245369353677719), ('event association', 0.10151615595853472), ('biochemical events', 0.10151545595853471), ('several classifiers', 0.0917439292649624), ('biomedical texts', 0.0917433192649624), ('biological context', 0.0917432192649624)] \n\n[('open source tool', 0.2210428614670275), ('natural language processing', 0.20066946630249308), ('source code', 0.17452782453073104), ('single command', 0.13377969420166205), ('wikipedia dump', 0.13377964420166205), ('quality embeddings', 0.13377956420166207), ('languages', 0.09745135984763284), ('wikipedia', 0.06688981210083103), ('embeddings', 0.06688974210083103), ('tool', 0.04651593693629646)] \n\n[('non - observed ones', 0.06896610722660707), ('low dimensional vector space', 0.06896587722660708), ('negative sampling', 0.05186291030064431), ('generative adversarial network', 0.051724927919955296), ('training data', 0.044767707591905535), ('art negative', 0.04189778686395074), ('negative triplets', 0.04189551686395074), ('important questions', 0.03819696761670803), ('important step', 0.03819579761670803), ('various kg', 0.03448543861330353)] \n\n[('english subtask a', 0.11884328896239889), ('automatic misogyny identification', 0.10791404906474815), ('machine learning models', 0.10791398906474815), ('such harmful content', 0.10791386906474816), ('subtask b.', 0.09380081583846696), ('bow vectors', 0.08013962096640352), ('idf vectors', 0.08013958096640351), ('public use', 0.07194357604316541), ('sentence embeddings', 0.0719430360431654), ('urgent need', 0.07194256604316542)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('conditional bert contextual augmentation', 0.18745129641720287), ('conditional masked language model', 0.17670879612467263), ('deep bidirectional language model', 0.14909651505476265), ('various different text classification tasks', 0.14662942597845632), ('masked language model', 0.12109322931212022), ('contextual augmentation augments', 0.12086360481673163), ('novel data augmentation method', 0.11444277054281826), ('conditional bert', 0.11236815396973727), ('unidirectional language model', 0.10925064113662006), ('data augmentation methods', 0.09596676904458763)] \n\n[('deep latent variable', 0.13660748368590403), ('latent variable models', 0.12025122338342067), ('posterior inference intractable', 0.1035787632701033), ('latent variable objectives', 0.10329498934632711), ('non - differentiability', 0.09389781361502346), ('powerful function approximators', 0.09389725361502345), ('variational inference', 0.08196130838684205), ('deep parameterizations', 0.07171002612961813), ('deep learning', 0.07170937612961814), ('world phenomena', 0.06259859907668229)] \n\n[('external convolutional language model', 0.13910843890799893), ('art speech recognition systems', 0.13138509205291768), ('times more acoustic data', 0.12063729554379947), ('convolutional neural networks', 0.11552568542748626), ('convolutional approach', 0.0849454040106537), ('wall street journal', 0.08356658961002786), ('feature extraction step', 0.08356639961002786), ('acoustic models', 0.08322496379154354), ('language modeling', 0.06866128541843916), ('deep speech', 0.0646846101094902)] \n\n[('domain synthetic data', 0.1232299107567185), ('multi - task learning', 0.11869537198830282), ('sequence neural network architectures', 0.11869444198830283), ('natural language understanding component', 0.11223984387619294), ('domain real data', 0.1081712372511297), ('real natural language', 0.09971533480744237), ('natural language understanding', 0.09356046856384494), ('enough training data', 0.08922769262162233), ('airline travel information', 0.08902214149122711), ('smaller synthetic dataset', 0.0872673267053069)] \n\n[('dialog system technology challenge', 0.11311542137108042), ('audio visual scene', 0.09078028436433305), ('multi - modal', 0.06666768666656121), ('baseline model', 0.05777063003842395), ('vggish models', 0.05776992003842395), ('decoder model', 0.05776946003842395), ('audio frames', 0.05248349367696474), ('visual content', 0.05248226367696474), ('understanding audio', 0.052482233676964744), ('intelligent systems', 0.05002591363659342)] \n\n[('novel model aggregation', 0.11851361205650361), ('model aggregation', 0.1060645589849835), ('server model', 0.10513754199512088), ('centralized machine learning technique', 0.09628984828238075), ('client models', 0.08672672776875089), ('global model', 0.0867266077687509), ('training models', 0.08672643776875089), ('modeling problem', 0.08672590776875089), ('model', 0.07427885469723078), ('social media dataset', 0.06912656396313224)] \n\n[('edi query intent prediction dataset', 0.16718409460440126), ('predicting user behaviour', 0.10242455428022515), ('user intent', 0.0855340891958391), ('ground truth dataset', 0.08524514715443524), ('convolutional networks', 0.06250015090437301), ('automatic labeling', 0.062500010904373), ('various rule', 0.06249976090437301), ('competitive classifiers', 0.062499410904373005), ('word information', 0.062499280904373004), ('domain detection', 0.062499150904373)] \n\n[('enablement attention mechanism', 0.1726798083381678), ('overall domain classification performance', 0.15722279382237261), ('domain enablement information', 0.1570639828850555), ('scale domain classification', 0.1537363465209072), ('attention weighting', 0.14021484112098104), ('attention information', 0.1367554031068725), ('supervised attention', 0.12251638756930626), ('softmax attention', 0.12251609756930626), ('attention mechanism', 0.12251554756930626), ('natural language understanding', 0.11428579426348613)] \n\n[('conversational recommendation systems', 0.08558953260942213), ('new neural architectures', 0.07333026946358609), ('overall problem domain', 0.06666858666656121), ('recommendation generation', 0.06145826369194393), ('movie recommendations', 0.06145743369194393), ('conversational recommendation', 0.06145641369194393), ('neural networks', 0.05776857003842395), ('world dialogues', 0.050608995482330395), ('such sub', 0.04944323154214282), ('model sub', 0.04944278154214282)] \n\n[('source speech recognition systems', 0.14206072261591735), ('speech recognition framework', 0.12313749392976399), ('other optimized frameworks', 0.0996960522030634), ('arrayfire tensor library', 0.09063473108754627), ('source deep learning', 0.08670843555304847), ('speech recognition', 0.07633866535371607), ('training times', 0.06721348701801103), ('training end', 0.06721328701801102), ('performance frameworks', 0.06464328911139179), ('model tuning', 0.06042433072503083)] \n\n[('more input modalities', 0.11535223911454359), ('modality translations', 0.11267706472663824), ('input modalities', 0.10175002538618431), ('multimodal sentiment analysis datasets', 0.09960290540165319), ('perturbed modalities', 0.09029652224157067), ('other modalities', 0.09029583224157067), ('source modality', 0.09029557224157067), ('target modality', 0.09029496224157067), ('acoustic modalities', 0.09029358224157066), ('multimodal sentiment analysis', 0.08567077642741384)] \n\n[('multi - head attention', 0.10654114933183553), ('model characteristics such', 0.08656951621128153), ('sequence position information', 0.0787419074803086), ('basic building block', 0.0787408374803086), ('past few years', 0.0787403174803086), ('neural network approaches', 0.0771877147888845), ('attention networks', 0.07320407143251932), ('neural networks', 0.06065611042150121), ('modifications such', 0.052495038320205724), ('various san', 0.052495008320205705)] \n\n[('several different segmentation criteria', 0.19283768516963265), ('criteria chinese word segmentation', 0.18631617479247073), ('flexible multi - criteria', 0.1584306769891903), ('heterogeneous segmentation criteria', 0.14464807164179766), ('multi - criteria cws', 0.13887634807447816), ('chinese word segmentation', 0.12943824816427024), ('term memory neural networks', 0.12902201533369456), ('segmentation criteria', 0.1277871577297768), ('segmentation criterion', 0.10384559050119208), ('word segmentation', 0.10015864293607869)] \n\n[('different pre - processing strategies', 0.10086421858765056), ('standardized project gutenberg corpus', 0.07664188167679699), ('other major linguistic datasets', 0.06875064100724897), ('complete pg data', 0.06448563606921139), ('most pg studies', 0.06448481606921139), ('pre - processed', 0.059930702253170515), ('consensual full version', 0.057966101799890105), ('corpus linguistics', 0.05410107582223154), ('new scientific resource', 0.052726857327022876), ('potential biased subsets', 0.05272484732702287)] \n\n[('future research topics', 0.09854419578456607), ('probabilistic topic modeling', 0.08467779431602607), ('latent dirichlet allocation', 0.07220274606495003), ('lda topics', 0.06939034553424805), ('topic modeling', 0.06938993553424806), ('topics', 0.05410298675247006), ('semantic framework', 0.053543386505429245), ('semantic mining', 0.053543176505429245), ('internet portals', 0.053542146505429246), ('internet society', 0.053541936505429245)] \n\n[('7th dialog system technology challenges', 0.17827261369520983), ('dialog system technologies', 0.1388236762020029), ('important contextual feature', 0.09288035647720652), ('intelligent virtual assistants', 0.09287933647720652), ('audio visual', 0.08038759354157306), ('aware dialog', 0.07127998887634464), ('baseline system', 0.07007625341116112), ('model variations', 0.06192108431813765), ('detailed analysis', 0.06192096431813765), ('classification convnet', 0.061920854318137646)] \n\n[('neural machine translation', 0.12726734740097695), ('best tokenizer differs', 0.12110540323240501), ('final translation quality', 0.10706883087738184), ('different language pairs', 0.10379991050735887), ('sophisticated processes', 0.090012041611711), ('simple processes', 0.09001192161171101), ('final translation', 0.08434629374022012), ('translation quality', 0.08434595374022012), ('splitting tokens', 0.08135598039673914), ('word embeddings', 0.06908515867012018)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('associated training data consists', 0.11811418295102571), ('caption data', 0.08334931745847626), ('training data', 0.08334733745847626), ('truth object detections', 0.07825179396185536), ('alternative data sources', 0.07600846554664911), ('object classes', 0.07560246552349273), ('open images image', 0.0749615876380453), ('limited visual concepts', 0.06818193818177588), ('many more classes', 0.06690351639276874), ('novel object', 0.06431094522566906)] \n\n[('robust structural representations', 0.15704979097131203), ('rnn autoencoder representations', 0.1377812346117322), ('tensor product representations', 0.13506021773408644), ('continuous vector representations', 0.12629768842240993), ('sentence representation learning', 0.11291875126796211), ('sequence representations', 0.11210879329969066), ('vector representations', 0.11210762329969066), ('sensitive representations', 0.10010730750443916), ('tensor product decomposition networks', 0.09746600884591206), ('interpretable compositional structure', 0.09638963340186942)] \n\n[('industrial scale', 0.1252941831041901), ('production workflows', 0.1252939631041901), ('own experience', 0.12529389310419012), ('execution engine', 0.12529383310419012), ('model components', 0.12529363310419012), ('extensible interfaces', 0.1252936031041901), ('deep learning', 0.1252932231041901), ('scale', 0.06264692155209506), ('models', 0.06264690155209506), ('ideas', 0.009458477559282218)] \n\n[('neural language processing', 0.13624927465218398), ('neural network models', 0.1354488582772353), ('natural language processing', 0.11737840556612633), ('neural networks', 0.08862022083710598), ('future work', 0.07968240385596664), ('potential directions', 0.07968236385596664), ('prominent research', 0.07968224385596664), ('analysis methods', 0.07968212385596664), ('survey paper', 0.07968207385596664), ('rich counterparts', 0.07968178385596664)] \n\n[('machine social chat', 0.09988944319947564), ('popular social chatbot', 0.09988884319947565), ('term user engagement', 0.08615082164212079), ('microsoft xiaoice system', 0.0749602076380453), ('social belonging', 0.07409233711567877), ('user needs', 0.07084548910294036), ('scale online logs', 0.06818379818177588), ('empathetic computing module', 0.06818316818177587), ('markov decision processes', 0.06818264818177587), ('human conversations', 0.06188791449436071)] \n\n[('natural language instructions', 0.08905805517626096), ('novel qweb neural network', 0.08546316294039327), ('synthetic instructions', 0.06560982553700423), ('new instruction', 0.06560942553700423), ('complicated instruction', 0.06560879553700423), ('possible instructions', 0.06560836553700423), ('several difficult environments', 0.05905823809567432), ('% success rate', 0.05905819809567432), ('instructions', 0.053424542941926105), ('rl approaches', 0.05111333187831263)] \n\n[('neural machine translation', 0.10953747023890296), ('linguistic correlation method', 0.10821362002638246), ('linguistic correlation analysis', 0.10821204002638246), ('end neural models', 0.10579358536302254), ('deep neural networks', 0.09611679640830977), ('neural language', 0.09504032877052879), ('model correlation analysis', 0.09401646821447635), ('dalvi et al', 0.07117693722401348), ('different linguistic properties', 0.0654093686797751), ('overall task', 0.06223526337009251)] \n\n[('neural network models', 0.22176410123945878), ('data biases', 0.13448285396703114), ('model distillation', 0.13448281396703107), ('research directions', 0.13448267396703115), ('external task', 0.13448214396703115), ('several methods', 0.13448199396703114), ('model', 0.08728248727242771), ('choices', 0.010164644010411216), ('architectural', 0.010164624010411216), ('such', 0.010164554010411216)] \n\n[('new ner problem settings', 0.11083132458517204), ('shelf ner tools', 0.09108220101864702), ('early ner systems', 0.09108118101864701), ('ner corpora', 0.08116069823656186), ('ner resources', 0.08116063823656186), ('ner systems', 0.08116028823656186), ('ner', 0.07123772545447671), ('natural language applications such', 0.07061591769091606), ('named entity recognition', 0.06289181265441811), ('decent recognition accuracy', 0.05286756436730259)] \n\n[('other alternative model architectures', 0.15291914727142783), ('neural network model', 0.13451745690113626), ('natural language understanding services', 0.1219528395121859), ('slot filling performance', 0.10055751855603114), ('natural language understanding', 0.10055648855603114), ('effective modeling', 0.09873579625906691), ('joint models', 0.0987354362590669), ('model', 0.08033537588877533), ('slot filling', 0.07916163759987638), ('level intent', 0.06781825008349719)] \n\n[('traditional semantic similarity models', 0.16361274866742617), ('semantic similarity scores', 0.143573986571589), ('similar tweets', 0.09007852732693773), ('semantic understanding', 0.07353741134048847), ('embedding space', 0.0711835012542785), ('sentence embeddings', 0.0711830212542785), ('sparse datasets', 0.0711827312542785), ('textual datasets', 0.0711824412542785), ('similarity', 0.07043856456020789), ('pure text', 0.06369572751334947)] \n\n[('cross - architecture binary code analysis', 0.1804494355600111), ('other cross - architecture binary code', 0.1638137910316846), ('cross - architecture binary code', 0.1500275221499261), ('cross - architecture instruction', 0.1198203290335515), ('binary code analysis', 0.09212261443005958), ('code analysis', 0.07145059975773911), ('many nlp tasks', 0.06942836072928814), ('joint learning approach', 0.06689159210683188), ('source code', 0.06428349269135726), ('basic block comparison', 0.06320282992941786)] \n\n[('non - technical documents such', 0.11786388552664419), ('pre - trained word vectors', 0.10980970705841528), ('art sentiment analysis techniques', 0.08742817398898248), ('software engineering research community', 0.08046791691807852), ('sentiment analysis tools', 0.06501165223678187), ('re -', 0.057126114690751593), ('gold standard datasets', 0.055608683558850805), ('convolutional neural networks', 0.055608183558850804), ('sentiment analysis', 0.05334167897440432), ('developer opinions', 0.048128093562726536)] \n\n[('asp system dlv', 0.08878397699236547), ('input logic program', 0.08333288073661019), ('answer set programming', 0.08333205073661018), ('extensive experimental activity', 0.08075602120987589), ('grounding subsystem i', 0.0807559112098759), ('different preference policies', 0.0807554112098759), ('proper new heuristics', 0.0807550612098759), ('asp system', 0.0698925423715631), ('logic programming', 0.06527608847756149), ('new heuristics', 0.06012105942409289)] \n\n[('standard local cross - entropy training', 0.1712472717272498), ('policy gradient training', 0.09939981892004748), ('neural sequence models', 0.0923268908092139), ('external prior knowledge', 0.08673245024613833), ('certain predefined features', 0.08622971527577229), ('model distribution', 0.07523728800399647), ('prior assumptions', 0.06889114709510563), ('prior knowledge', 0.06889001709510563), ('sequential model', 0.06070995735221557), ('stochastic gradients', 0.05873557167131351)] \n\n[('speech commands classification model', 0.13059489065274352), ('speech commands model', 0.11090379813839826), ('vocabulary speech classifiers', 0.07671005673775547), ('computer vision systems', 0.06534862293709699), ('many industrial systems', 0.065348382937097), ('speech classifiers', 0.06471687175065874), ('speech recognition', 0.06471530175065873), ('speech processing', 0.06471520175065874), ('different frequency bands', 0.06336070228032052), ('audio space', 0.05556232513132267)] \n\n[('neural machine translation', 0.12750409442134358), ('entire translation process', 0.11127867073799898), ('translation performance', 0.09413663400021144), ('translation tasks', 0.09413651400021143), ('reinforcement learning models', 0.08403496344535445), ('translation', 0.0769920172624239), ('source sentence', 0.07273145674408465), ('refining strategy', 0.062538546152483), ('standard encoder', 0.06253844615248301), ('refining operations', 0.062537836152483)] \n\n[('multilingual knowledge extraction corpus', 0.09950750932242822), ('knowledge extraction challenge', 0.07557056670669818), ('% more links', 0.07445461717095772), ('nlp interchange format', 0.07445427717095775), ('semi - structured', 0.06768829105075778), ('wikipedia article texts', 0.06568834155645552), ('structured information', 0.06484401122180769), ('valuable information', 0.05345540191260173), ('categorization information', 0.05345524191260173), ('wikipedia articles', 0.05111330112711626)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('global anchor method recovers', 0.12222159627487433), ('global anchor method', 0.10379303645975081), ('disparate text corpora', 0.0843901956539987), ('graph laplacian technique', 0.08439002565399871), ('level language shifts', 0.0843888556539987), ('distributional tools such', 0.0843886356539987), ('active research area', 0.0843884356539987), ('alignment method', 0.0688399949647354), ('domain clustering', 0.06280454524494689), ('domain adaptation', 0.06280432524494689)] \n\n[('step random walks', 0.11811124621031645), ('most current word', 0.11811059621031646), ('natural language processing', 0.11811040621031645), ('various queries', 0.07874157747354428), ('experiment results', 0.07874136747354428), ('input neighborhoods', 0.07874105747354428), ('minimal discrepancy', 0.07874101747354428), ('novel neighbor', 0.07874077747354429), ('many text', 0.07874039747354428), ('important starting', 0.07874027747354428)] \n\n[('language speech emotion database', 0.20925917398703983), ('cross - lingual emotion recognition', 0.17286046093582683), ('lingual speech emotion recognition', 0.1662986326454596), ('automatic speech emotion recognition systems', 0.1614584469281196), ('different western languages', 0.1410688654169332), ('such limited languages', 0.13859719235951926), ('speech emotion recognition', 0.13197124764196375), ('urdu language', 0.13075541803609486), ('unseen language such', 0.12844079180356072), ('adaptive emotion recognition system', 0.11676755469865711)] \n\n[('cross - lingual natural language inference', 0.1519838786339272), ('pre - trained encoder', 0.08555000723732101), ('parallel corpus mining', 0.07971128087554477), ('joint multilingual sentence', 0.07691896107500933), ('cross - lingual', 0.07140961016463636), ('different language', 0.07112197271361924), ('multilingual test set', 0.06727453070382067), ('resource languages', 0.06088048233985928), ('language pairs', 0.060880052339859284), ('english annotated data', 0.058537405365853085)] \n\n[('process mining techniques', 0.11737570876811304), ('dialogue system transcripts', 0.10678657768084698), ('structured event logs', 0.10676208583627428), ('conversational transcripts', 0.10215290832832603), ('conversation flow', 0.08412483055417519), ('multiple conversational', 0.0841242605541752), ('qrfa model', 0.07945252729809653), ('new model', 0.07945203729809652), ('conformance analysis', 0.07117628722418287), ('different domains', 0.07117568722418288)] \n\n[('multi - aspect sentiment analysis', 0.2444509222860123), ('aspect level sentiment classification', 0.24340018580077905), ('sentence level classification', 0.14345211264094235), ('multi - task settings', 0.13465633494510576), ('different aspects', 0.1297125825267091), ('multiple aspects', 0.11190491586915409), ('more aspects', 0.11190439586915409), ('specific sentence representations', 0.09399727790900121), ('aspect', 0.09327667842153706), ('sentiment analysis', 0.07807762541717793)] \n\n[('automatic clickbait detection', 0.1178073871053247), ('better clickbait detectors', 0.10342843662675372), ('social media users', 0.09599457965987161), ('clickbait detectors', 0.08728249095787806), ('clickbait challenge', 0.08728237095787807), ('good starting point', 0.07843275254901955), ('machine learning technology', 0.07843179254901955), ('malicious content publishers', 0.07843156254901955), ('clickbait', 0.0711365952890024), ('social media', 0.0638338327567671)] \n\n[('reusable neural network components', 0.12121226119455804), ('scale systematic study', 0.09090962089591852), ('contextualized word representation', 0.09090915089591851), ('sentence encoders', 0.06744183446292502), ('varied results', 0.06744164446292503), ('primary results', 0.06744114446292503), ('sentence understanding', 0.06744055446292502), ('further work', 0.060607620597279), ('ideal platform', 0.060607590597279), ('target tasks', 0.060607380597279)] \n\n[('several typical krl methods', 0.12966036175301898), ('knowledge graph completion', 0.1192535229374914), ('knowledge graph', 0.09760982787180704), ('dimensional semantic space', 0.09724493131476425), ('massive knowledge', 0.07897451310606678), ('knowledge representation', 0.07897421310606677), ('information retrieval', 0.06483099087650948), ('question answering', 0.06483096087650948), ('language modeling', 0.06483093087650948), ('world applications', 0.06483085087650949)] \n\n[('overall scheduling efficiency', 0.11165802732978883), ('standard scheduling algorithms', 0.11165795732978884), ('agreeable time slot', 0.11093081876746666), ('meeting arrival rate', 0.09964570260134124), ('preferred time', 0.07391414327476423), ('policy function', 0.06743149947246733), ('policy gradient', 0.06743134947246733), ('rl system', 0.06743107947246733), ('user utterences', 0.06743071947246733), ('multiple users', 0.06743058947246733)] \n\n[('hierarchical neural structure', 0.10835338154351229), ('model pre - training', 0.0992068668176783), ('deep neural models', 0.09796001940679919), ('such weak supervision signals', 0.08898931418242492), ('weak supervision signals such', 0.08898912418242493), ('deep neural networks', 0.08868196403363683), ('hierarchical text classification', 0.07904152308696584), ('neural method', 0.07639294794466618), ('real unlabeled data', 0.07262023902503424), ('text documents', 0.05879778680886386)] \n\n[('deep biaffine attention layer', 0.13698055957609592), ('entity recognition model', 0.12097937708155909), ('neural network model', 0.1209789970815591), ('previous models', 0.07946658680155669), ('art performances', 0.06849115978804796), ('new state', 0.06849108978804797), ('experimental results', 0.06849096978804796), ('dataset conll04', 0.06849093978804796), ('directional relationship', 0.06849080978804796), ('relation classification', 0.06849066978804796)] \n\n[('multi - label learning framework', 0.2005211527733615), ('multiple entity pairs relation extraction', 0.18691686785046724), ('novel neural approach', 0.11215005271028036), ('multi - instance', 0.1121038806657458), ('different benchmarks', 0.07476705514018687), ('attention mechanisms', 0.07476696514018687), ('capsule networks', 0.07476667514018687), ('specific type', 0.07476654514018687), ('instantiation parameters', 0.07476650514018687), ('activity vector', 0.07476645514018687)] \n\n[('vietnamese word segmentation', 0.15681031934580214), ('benchmark vietnamese datasets', 0.13191260437800478), ('huang et al', 0.12766011446808506), ('first joint model', 0.12765961446808508), ('joint model', 0.09480295851638099), ('dependency parser', 0.09480250851638099), ('dependency parsing', 0.094802388516381), ('competitive performances', 0.0851072929787234), ('experimental results', 0.08510712297872339), ('pos tagging', 0.08510704297872339)] \n\n[('multi - modal attention distributions', 0.2504659126590072), ('attention vector', 0.12109858159683487), ('attention mechanism', 0.12109851159683487), ('attention model', 0.12109814159683487), ('stance detection task', 0.11385309240986718), ('conventional deterministic counterpart', 0.11385270240986718), ('attention', 0.0986942412011584), ('latent variables', 0.08466864612232296), ('random variables', 0.08466801612232297), ('variational inference', 0.08466784612232296)] \n\n[('cross - language citation recommendation task', 0.1861312857095399), ('cross - language citation recommendation', 0.14547715879675746), ('important cross - language neighborhoods', 0.12952922577723439), ('foreign language repository', 0.09649155156433421), ('hierarchical representation learning', 0.07806252527916177), ('other languages', 0.07592249543101488), ('language barrier', 0.07592231543101488), ('hierarchical random walk algorithm', 0.07468955077855328), ('publication representations', 0.06840691150872598), ('representation model', 0.06764310583819709)] \n\n[('non - trivial inference mechanisms', 0.174216187860493), ('strategy deep learning models', 0.1393733222883944), ('approximate number system', 0.10453069671629579), ('visual question', 0.1000354268334198), ('same question', 0.08240182105757345), ('such questions', 0.08240162105757345), ('visual scene', 0.08167394464128104), ('spatial arrangement', 0.0696877711441972), ('cognitive concepts', 0.0696868211441972), ('quantifier statements', 0.0696864611441972)] \n\n","name":"stdout"},{"output_type":"stream","text":"[('peters et al', 0.14679619063095162), ('pre - training', 0.13333352333333215), ('devlin et', 0.11581431348412706), ('art results', 0.08888986888888806), ('new state', 0.08888979888888807), ('model capacity', 0.08888972888888806), ('large part', 0.08888964888888806), ('outperforms elmo', 0.08888961888888806), ('additional languages', 0.08888912888888806), ('previous work', 0.08888892888888807)] \n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#titles & keyphrases\n\npapers.loc[:,['Title','Abstract_Keyphrases']]","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"                                                 Title                                Abstract_Keyphrases\n0    A Deep Sequential Model for Discourse Parsing ...  [(discourse dependency structures, 0.128116458...\n1    Learning Speaker Representations with Mutual I...  [(useful speaker representations, 0.0697218003...\n2    Fake News: A Survey of Research, Detection Met...  [(current fake news research, 0.15535480924421...\n3    A Study on Dialogue Reward Prediction for Open...  [(training dialogue reward predictors, 0.17872...\n4    Improved and Robust Controversy Detection in G...  [(cross - domain performance, 0.09234450374810...\n5       Learning Representations of Social Media Users  [(distant user information, 0.0709560415423658...\n6    Clinical Document Classification Using Labeled...  [(supervised learning pipeline, 0.094264009320...\n7    Building Sequential Inference Models for End-t...  [(general pre - trained word embeddings, 0.156...\n8    Toward Scalable Neural Dialogue State Tracking...  [(accurate neural dialogue state tracking mode...\n9                         A Survey on Semantic Parsing  [(semi - structured knowledge bases, 0.1503760...\n10   e-SNLI: Natural Language Inference with Natura...  [(stanford natural language inference dataset,...\n11   Practical Text Classification With Large Pre-T...  [(real world text classification, 0.0827443120...\n12   Tartan: A retrieval-based socialbot powered by...  [(fluent casual conversation, 0.19612404018624...\n13   Modeling natural language emergence with integ...  [(communicative bias functions, 0.120745090235...\n14   Leveraging Multi-grained Sentiment Lexicon Inf...  [(neural sequence models, 0.09588259444178886)...\n15   Impact of Sentiment Detection to Recognize Tox...  [(toxicity detection tool, 0.15189410121117256...\n16   MedSim: A Novel Semantic Similarity Measure in...  [(other semantic similarity methods, 0.1416668...\n17                   Summarizing Videos with Attention  [(directional recurrent networks such, 0.17094...\n18   Inflection-Tolerant Ontology-Based Named Entit...  [(sophisticated natural language processing, 0...\n19   Weighted Global Normalization for Multiple Cho...  [(mean reciprocal rank, 0.12588833010767325), ...\n20   Neural Abstractive Text Summarization with Seq...  [(several different neural network components,...\n21   EvoMSA: A Multilingual Evolutionary Approach f...  [(different sa competitions, 0.158705235809570...\n22   Multi-Task Learning with Multi-View Attention ...  [(multi - view attention scheme, 0.15558861065...\n23   The USTC-NEL Speech Translation system at IWSL...  [(speech recognition output style text, 0.2035...\n24   Evaluating Architectural Choices for Deep Lear...  [(various deep learning architectures, 0.12037...\n25   Generation of Synthetic Electronic Medical Rec...  [(emr data analysis, 0.08370093873014466), (te...\n26               End-to-End Streaming Keyword Spotting  [(neural network topology, 0.20049000155238553...\n27       Political Popularity Analysis in Social Media  [(senator bernie sanders, 0.09788022221859581)...\n28   Dialogue Generation: From Imitation Learning t...  [(adversarial dialogue generation models, 0.15...\n29   SDNet: Contextualized Attention-based Deep Net...  [(latest bert contextual model, 0.130862652625...\n..                                                 ...                                                ...\n76   What Is One Grain of Sand in the Desert? Analy...  [(neural machine translation, 0.10953747023890...\n77   NeuroX: A Toolkit for Analyzing Individual Neu...  [(neural network models, 0.22176410123945878),...\n78   A Survey on Deep Learning for Named Entity Rec...  [(new ner problem settings, 0.1108313245851720...\n79   Joint Slot Filling and Intent Detection via Ca...  [(other alternative model architectures, 0.152...\n80   Improving Context-Aware Semantic Relationships...  [(traditional semantic similarity models, 0.16...\n81   A Cross-Architecture Instruction Embedding Mod...  [(cross - architecture binary code analysis, 0...\n82   Supervised Sentiment Classification with CNNs ...  [(non - technical documents such, 0.1178638855...\n83   Optimizing Answer Set Computation via Heuristi...  [(asp system dlv, 0.08878397699236547), (input...\n84   Moment Matching Training for Neural Machine Tr...  [(standard local cross - entropy training, 0.1...\n85   Noise Flooding for Detecting Audio Adversarial...  [(speech commands classification model, 0.1305...\n86   Learning to Refine Source Representations for ...  [(neural machine translation, 0.12750409442134...\n87   DBpedia NIF: Open, Large-Scale and Multilingua...  [(multilingual knowledge extraction corpus, 0....\n88   The Global Anchor Method for Quantifying Lingu...  [(global anchor method recovers, 0.12222159627...\n89   Word Embedding based on Low-Rank Doubly Stocha...  [(step random walks, 0.11811124621031645), (mo...\n90   Cross Lingual Speech Emotion Recognition: Urdu...  [(language speech emotion database, 0.20925917...\n91   Massively Multilingual Sentence Embeddings for...  [(cross - lingual natural language inference, ...\n92   QRFA: A Data-Driven Model of Information-Seeki...  [(process mining techniques, 0.117375708768113...\n93   CAN: Constrained Attention Networks for Multi-...  [(multi - aspect sentiment analysis, 0.2444509...\n94   The Clickbait Challenge 2017: Towards a Regres...  [(automatic clickbait detection, 0.11780738710...\n95   Looking for ELMo's Friends: Sentence-Level Pre...  [(reusable neural network components, 0.121212...\n96   Knowledge Representation Learning: A Quantitat...  [(several typical krl methods, 0.1296603617530...\n97   MEETING BOT: Reinforcement Learning for Dialog...  [(overall scheduling efficiency, 0.11165802732...\n98   Weakly-Supervised Hierarchical Text Classifica...  [(hierarchical neural structure, 0.10835338154...\n99   End-to-end neural relation extraction using de...  [(deep biaffine attention layer, 0.13698055957...\n100  Attention-Based Capsule Networks with Dynamic ...  [(multi - label learning framework, 0.20052115...\n101  A neural joint model for Vietnamese word segme...  [(vietnamese word segmentation, 0.156810319345...\n102  Variational Self-attention Model for Sentence ...  [(multi - modal attention distributions, 0.250...\n103  Cross-language Citation Recommendation via Hie...  [(cross - language citation recommendation tas...\n104  The meaning of \"most\" for visual question answ...  [(non - trivial inference mechanisms, 0.174216...\n105  Multilingual Constituency Parsing with Self-At...  [(peters et al, 0.14679619063095162), (pre - t...\n\n[106 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Abstract_Keyphrases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Deep Sequential Model for Discourse Parsing ...</td>\n      <td>[(discourse dependency structures, 0.128116458...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Learning Speaker Representations with Mutual I...</td>\n      <td>[(useful speaker representations, 0.0697218003...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Fake News: A Survey of Research, Detection Met...</td>\n      <td>[(current fake news research, 0.15535480924421...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A Study on Dialogue Reward Prediction for Open...</td>\n      <td>[(training dialogue reward predictors, 0.17872...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Improved and Robust Controversy Detection in G...</td>\n      <td>[(cross - domain performance, 0.09234450374810...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Learning Representations of Social Media Users</td>\n      <td>[(distant user information, 0.0709560415423658...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Clinical Document Classification Using Labeled...</td>\n      <td>[(supervised learning pipeline, 0.094264009320...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Building Sequential Inference Models for End-t...</td>\n      <td>[(general pre - trained word embeddings, 0.156...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Toward Scalable Neural Dialogue State Tracking...</td>\n      <td>[(accurate neural dialogue state tracking mode...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A Survey on Semantic Parsing</td>\n      <td>[(semi - structured knowledge bases, 0.1503760...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>e-SNLI: Natural Language Inference with Natura...</td>\n      <td>[(stanford natural language inference dataset,...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Practical Text Classification With Large Pre-T...</td>\n      <td>[(real world text classification, 0.0827443120...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Tartan: A retrieval-based socialbot powered by...</td>\n      <td>[(fluent casual conversation, 0.19612404018624...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Modeling natural language emergence with integ...</td>\n      <td>[(communicative bias functions, 0.120745090235...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Leveraging Multi-grained Sentiment Lexicon Inf...</td>\n      <td>[(neural sequence models, 0.09588259444178886)...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Impact of Sentiment Detection to Recognize Tox...</td>\n      <td>[(toxicity detection tool, 0.15189410121117256...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>MedSim: A Novel Semantic Similarity Measure in...</td>\n      <td>[(other semantic similarity methods, 0.1416668...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Summarizing Videos with Attention</td>\n      <td>[(directional recurrent networks such, 0.17094...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Inflection-Tolerant Ontology-Based Named Entit...</td>\n      <td>[(sophisticated natural language processing, 0...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Weighted Global Normalization for Multiple Cho...</td>\n      <td>[(mean reciprocal rank, 0.12588833010767325), ...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Neural Abstractive Text Summarization with Seq...</td>\n      <td>[(several different neural network components,...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>EvoMSA: A Multilingual Evolutionary Approach f...</td>\n      <td>[(different sa competitions, 0.158705235809570...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Multi-Task Learning with Multi-View Attention ...</td>\n      <td>[(multi - view attention scheme, 0.15558861065...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>The USTC-NEL Speech Translation system at IWSL...</td>\n      <td>[(speech recognition output style text, 0.2035...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Evaluating Architectural Choices for Deep Lear...</td>\n      <td>[(various deep learning architectures, 0.12037...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Generation of Synthetic Electronic Medical Rec...</td>\n      <td>[(emr data analysis, 0.08370093873014466), (te...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>End-to-End Streaming Keyword Spotting</td>\n      <td>[(neural network topology, 0.20049000155238553...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Political Popularity Analysis in Social Media</td>\n      <td>[(senator bernie sanders, 0.09788022221859581)...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Dialogue Generation: From Imitation Learning t...</td>\n      <td>[(adversarial dialogue generation models, 0.15...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>SDNet: Contextualized Attention-based Deep Net...</td>\n      <td>[(latest bert contextual model, 0.130862652625...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>What Is One Grain of Sand in the Desert? Analy...</td>\n      <td>[(neural machine translation, 0.10953747023890...</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>NeuroX: A Toolkit for Analyzing Individual Neu...</td>\n      <td>[(neural network models, 0.22176410123945878),...</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>A Survey on Deep Learning for Named Entity Rec...</td>\n      <td>[(new ner problem settings, 0.1108313245851720...</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Joint Slot Filling and Intent Detection via Ca...</td>\n      <td>[(other alternative model architectures, 0.152...</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Improving Context-Aware Semantic Relationships...</td>\n      <td>[(traditional semantic similarity models, 0.16...</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>A Cross-Architecture Instruction Embedding Mod...</td>\n      <td>[(cross - architecture binary code analysis, 0...</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>Supervised Sentiment Classification with CNNs ...</td>\n      <td>[(non - technical documents such, 0.1178638855...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Optimizing Answer Set Computation via Heuristi...</td>\n      <td>[(asp system dlv, 0.08878397699236547), (input...</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>Moment Matching Training for Neural Machine Tr...</td>\n      <td>[(standard local cross - entropy training, 0.1...</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>Noise Flooding for Detecting Audio Adversarial...</td>\n      <td>[(speech commands classification model, 0.1305...</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Learning to Refine Source Representations for ...</td>\n      <td>[(neural machine translation, 0.12750409442134...</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>DBpedia NIF: Open, Large-Scale and Multilingua...</td>\n      <td>[(multilingual knowledge extraction corpus, 0....</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>The Global Anchor Method for Quantifying Lingu...</td>\n      <td>[(global anchor method recovers, 0.12222159627...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Word Embedding based on Low-Rank Doubly Stocha...</td>\n      <td>[(step random walks, 0.11811124621031645), (mo...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>Cross Lingual Speech Emotion Recognition: Urdu...</td>\n      <td>[(language speech emotion database, 0.20925917...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Massively Multilingual Sentence Embeddings for...</td>\n      <td>[(cross - lingual natural language inference, ...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>QRFA: A Data-Driven Model of Information-Seeki...</td>\n      <td>[(process mining techniques, 0.117375708768113...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>CAN: Constrained Attention Networks for Multi-...</td>\n      <td>[(multi - aspect sentiment analysis, 0.2444509...</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>The Clickbait Challenge 2017: Towards a Regres...</td>\n      <td>[(automatic clickbait detection, 0.11780738710...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Looking for ELMo's Friends: Sentence-Level Pre...</td>\n      <td>[(reusable neural network components, 0.121212...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>Knowledge Representation Learning: A Quantitat...</td>\n      <td>[(several typical krl methods, 0.1296603617530...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>MEETING BOT: Reinforcement Learning for Dialog...</td>\n      <td>[(overall scheduling efficiency, 0.11165802732...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>Weakly-Supervised Hierarchical Text Classifica...</td>\n      <td>[(hierarchical neural structure, 0.10835338154...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>End-to-end neural relation extraction using de...</td>\n      <td>[(deep biaffine attention layer, 0.13698055957...</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>Attention-Based Capsule Networks with Dynamic ...</td>\n      <td>[(multi - label learning framework, 0.20052115...</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>A neural joint model for Vietnamese word segme...</td>\n      <td>[(vietnamese word segmentation, 0.156810319345...</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>Variational Self-attention Model for Sentence ...</td>\n      <td>[(multi - modal attention distributions, 0.250...</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>Cross-language Citation Recommendation via Hie...</td>\n      <td>[(cross - language citation recommendation tas...</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>The meaning of \"most\" for visual question answ...</td>\n      <td>[(non - trivial inference mechanisms, 0.174216...</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>Multilingual Constituency Parsing with Self-At...</td>\n      <td>[(peters et al, 0.14679619063095162), (pre - t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>106 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"> #### Titles Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = papers['Title']","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles[1]","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'Learning Speaker Representations with Mutual Information'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ncounts = count_vectorizer.fit_transform(titles)\ntfidf_vectorizer = TfidfTransformer().fit(counts)\ntfidf_titles = tfidf_vectorizer.transform(counts)\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_titles","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"<106x479 sparse matrix of type '<class 'numpy.float64'>'\n\twith 967 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Affinity Propogation\nX = tfidf_titles\nclustering = AffinityPropagation().fit(X)\nclustering \n\ncontent_affinity_clusters = list(clustering.labels_)\ncontent_affinity_clusters","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"[14,\n 1,\n 0,\n 10,\n 16,\n 1,\n 17,\n 18,\n 5,\n 14,\n 2,\n 17,\n 19,\n 2,\n 3,\n 0,\n 13,\n 19,\n 14,\n 7,\n 3,\n 16,\n 4,\n 11,\n 4,\n 17,\n 18,\n 1,\n 5,\n 10,\n 12,\n 6,\n 6,\n 18,\n 10,\n 3,\n 3,\n 7,\n 7,\n 3,\n 4,\n 10,\n 4,\n 19,\n 18,\n 15,\n 11,\n 14,\n 14,\n 19,\n 5,\n 6,\n 8,\n 9,\n 2,\n 11,\n 2,\n 4,\n 2,\n 7,\n 17,\n 10,\n 11,\n 15,\n 20,\n 6,\n 2,\n 6,\n 12,\n 15,\n 16,\n 1,\n 5,\n 0,\n 1,\n 5,\n 13,\n 13,\n 14,\n 19,\n 12,\n 2,\n 17,\n 4,\n 15,\n 8,\n 15,\n 16,\n 6,\n 6,\n 11,\n 20,\n 20,\n 19,\n 10,\n 20,\n 20,\n 5,\n 17,\n 18,\n 19,\n 6,\n 20,\n 17,\n 4,\n 20]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers['title_cluster'] = content_affinity_clusters","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check all papers in cluster 11\n\npapers_cluster11 = papers.loc[papers['title_cluster']==11,['Title','Abstract_Keyphrases']]","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_cluster11","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"                                                Title                                Abstract_Keyphrases\n23  The USTC-NEL Speech Translation system at IWSL...  [(speech recognition output style text, 0.2035...\n46  Speech and Speaker Recognition from Raw Wavefo...  [(novel convolutional neural network, 0.096922...\n55             Fully Convolutional Speech Recognition  [(external convolutional language model, 0.139...\n62  wav2letter++: The Fastest Open-source Speech R...  [(source speech recognition systems, 0.1420607...\n90  Cross Lingual Speech Emotion Recognition: Urdu...  [(language speech emotion database, 0.20925917...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Abstract_Keyphrases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23</th>\n      <td>The USTC-NEL Speech Translation system at IWSL...</td>\n      <td>[(speech recognition output style text, 0.2035...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Speech and Speaker Recognition from Raw Wavefo...</td>\n      <td>[(novel convolutional neural network, 0.096922...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>Fully Convolutional Speech Recognition</td>\n      <td>[(external convolutional language model, 0.139...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>wav2letter++: The Fastest Open-source Speech R...</td>\n      <td>[(source speech recognition systems, 0.1420607...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>Cross Lingual Speech Emotion Recognition: Urdu...</td>\n      <td>[(language speech emotion database, 0.20925917...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(sorted(papers_cluster11.values.tolist())) ","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"{'Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages': [('language speech emotion database',\n   0.20925917398703983),\n  ('cross - lingual emotion recognition', 0.17286046093582683),\n  ('lingual speech emotion recognition', 0.1662986326454596),\n  ('automatic speech emotion recognition systems', 0.1614584469281196),\n  ('different western languages', 0.1410688654169332),\n  ('such limited languages', 0.13859719235951926),\n  ('speech emotion recognition', 0.13197124764196375),\n  ('urdu language', 0.13075541803609486),\n  ('unseen language such', 0.12844079180356072),\n  ('adaptive emotion recognition system', 0.11676755469865711)],\n 'Fully Convolutional Speech Recognition': [('external convolutional language model',\n   0.13910843890799893),\n  ('art speech recognition systems', 0.13138509205291768),\n  ('times more acoustic data', 0.12063729554379947),\n  ('convolutional neural networks', 0.11552568542748626),\n  ('convolutional approach', 0.0849454040106537),\n  ('wall street journal', 0.08356658961002786),\n  ('feature extraction step', 0.08356639961002786),\n  ('acoustic models', 0.08322496379154354),\n  ('language modeling', 0.06866128541843916),\n  ('deep speech', 0.0646846101094902)],\n 'Speech and Speaker Recognition from Raw Waveform with SincNet': [('novel convolutional neural network',\n   0.09692220814259608),\n  ('raw audio samples', 0.07327073332947878),\n  ('deep neural networks', 0.0718042228445011),\n  ('neural architecture', 0.07040222005037888),\n  ('clear physical meaning', 0.06342697714567762),\n  ('high cutoff frequencies', 0.06342657714567762),\n  ('neural networks', 0.0588279728948894),\n  ('speech recognition', 0.05489626610103939),\n  ('audio waveforms', 0.05126957894268448),\n  ('raw waveform', 0.05126907894268448)],\n 'The USTC-NEL Speech Translation system at IWSLT 2018': [('speech recognition output style text',\n   0.2035788794578141),\n  ('conventional pipeline system', 0.1513004196435239),\n  ('speech translation task', 0.14853771775883792),\n  ('translation models', 0.1281205956679401),\n  ('machine translation', 0.12812050566794012),\n  ('post - processing', 0.1232036554209445),\n  ('baseline system', 0.10027071660502196),\n  ('nel system', 0.10026982660502197),\n  ('speech recognition', 0.08500910010123536),\n  ('translation', 0.08269865239563984)],\n 'wav2letter++: The Fastest Open-source Speech Recognition System': [('source speech recognition systems',\n   0.14206072261591735),\n  ('speech recognition framework', 0.12313749392976399),\n  ('other optimized frameworks', 0.0996960522030634),\n  ('arrayfire tensor library', 0.09063473108754627),\n  ('source deep learning', 0.08670843555304847),\n  ('speech recognition', 0.07633866535371607),\n  ('training times', 0.06721348701801103),\n  ('training end', 0.06721328701801102),\n  ('performance frameworks', 0.06464328911139179),\n  ('model tuning', 0.06042433072503083)]}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Above cluster seems to contain papers on speech processing. We can also see that top 3 keyphrases extracted using textrank algorithm have a good correspondence with paper titles."},{"metadata":{},"cell_type":"markdown","source":"_Thanks for reading this notebook. Please share your valuable feedback & upvote if you learn something new today from this analysis. Till now focus was on textrank algorithm only, I will also add comparision with other keyphrase extraction algorithms along with small description on how these graph-based keyphrase extraction algorithms work._"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}