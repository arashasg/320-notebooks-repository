{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"! pip install git+https://github.com/LIAAD/yake","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from brown_clustering_yangyuan import *\nimport pandas as pd\nfrom nltk.tokenize import RegexpTokenizer\nimport yake\n\n\nforum_posts = pd.read_csv(\"../input/meta-kaggle/ForumMessages.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def keywords_yake(sample_posts):\n    # take keywords for each post & turn them into a text string \"sentence\"\n    simple_kwextractor = yake.KeywordExtractor()\n\n    # create empty list to save our \"sentnecs\" to\n    sentences = []\n\n    for post in sample_posts:\n        post_keywords = simple_kwextractor.extract_keywords(post)\n\n        sentence_output = \"\"\n        for word, number in post_keywords:\n            sentence_output += word + \" \"\n\n        sentences.append(sentence_output)\n        \n    return(sentences)\n\ndef tokenizing_after_YAKE(sentences):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    sample_data_tokenized = [w.lower() for w in sentences]\n    sample_data_tokenized = [tokenizer.tokenize(i) for i in sample_data_tokenized]\n    \n    return(sample_data_tokenized)\n\ndef get_clusters_maybe(megacluster):\n    # so it looks like all the clustres have been concatenated to a single array\n    # but they're in alphabetical order so we can use that to un-cat them \n\n    # create list with one sub list\n    cluster_list = [[]] \n    list_index = 0\n\n    # look at all words but last (since we compare each word\n    # to the next word)\n    for i in range(len(megacluster) - 1):\n        if megacluster[i - 1] < megacluster[i]:\n            # add current word to current sublist\n            cluster_list[list_index].append(megacluster[i])\n        else:\n            # create a new sublist\n            cluster_list.append([])\n            list_index = list_index + 1\n\n            # add current word\n            cluster_list[list_index].append(megacluster[i])\n    \n    return(cluster_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# subsample forum posts\nsample_posts = forum_posts.Message[-1000:].astype(str)\n\n# extract keywords and token\nkeyword_output = keywords_yake(sample_posts)\ntokenized_output = tokenizing_after_YAKE(keyword_output)\n\n# cluster\ncorpus = Corpus(tokenized_output, 0.001)\nclustering = BrownClustering(corpus, 2)\nclustering.train()\n\n# seperate output into different clusters\nmegacluster = clustering.helper.get_cluster(0)\nclusters_maybe = get_clusters_maybe(megacluster)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_maybe[10:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rachael's note: I'm  not particiuarly happy with these clusters for my specific use case. Going forward I'm going to try a different approach. ="},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}