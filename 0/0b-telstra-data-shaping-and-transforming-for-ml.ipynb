{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"47c7ab75-c459-4c6c-bf58-1e902e458264","_uuid":"2a92fc961ebc68d0ad1ec0a2e413ffe3e4950b7f"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3a1448bf-3bfc-4bc6-8b92-f541a363a139","collapsed":true,"_uuid":"77f26fd2b5616051826ce0f146add09082fc0df2"},"source":"import pandas as pd\nimport numpy as np\nimport sys\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 1000)\nimport time","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"03addb30-44a9-4a8d-9cde-83e100c54f07","_uuid":"5dbbbb10def28cecb0f1510a0da3fb89a08ca3f4"},"source":"In additional to the train and test data, the Testra dataset come with several different types of information to do network incidents, these are event, resource, severity and log features. In this notebook, we try to reshape the original data into a format that can be used as input for machine learning pipeline  \n\nIn the dataset, location, event, resource, severity and log feature comes with value like \"location 1\", or \"event_type 1\", these categorical feature comes in string format. so the first we can we do is the input the data in away that we only keep the number following the word. This is done with a converter function during file reading.","cell_type":"markdown"},{"metadata":{"_cell_guid":"2ab4a651-466c-4f4f-a2b0-be2715048c9f","collapsed":true,"_uuid":"b06eddb2c7aee0a9f9e7c4752ca954142d4df3e1"},"source":"def str_to_num(string):\n    return int(string.split(\" \")[1])\n\ntrain=pd.read_csv('../input/train.csv', converters={'location':str_to_num})\ntest=pd.read_csv('../input/test.csv', converters={'location':str_to_num})\nevent=pd.read_csv('../input/event_type.csv', converters={'event_type':str_to_num})\nlog_feature=pd.read_csv('../input/log_feature.csv', converters={'log_feature':str_to_num})\nseverity=pd.read_csv('../input/severity_type.csv', converters={'severity_type':str_to_num})\nresource=pd.read_csv('../input/resource_type.csv', converters={'resource_type':str_to_num})","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"event.head(3)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"1df51695-d0db-4c5f-bbaa-f4ce8745890a","_uuid":"6ebef7ccd98e1f977a082596629e259344cfd0d6"},"source":"These data are inter-linked via the 'id' field. Each row in train and test data contain an unique id, the same with severity data. for resource, event and log feature, we have same id appearing multiple time in each file, basically the same id can have multiple resource types, event types and log features. For log feature we additional have a numerical volume value for a id and log feature pair. \n\nso the basicall strategy here is to shape the resource, event and log feature data in a way that each row corresponds on one unique id, in the same way as train, test and severity files. \n\nFor event and resrouce data, this can be done simply by performing one-hot encoding using the get_dummies method from pandas.","cell_type":"markdown"},{"metadata":{"_cell_guid":"2b4cde1f-638c-4353-874c-4c8ed6b53702","collapsed":true,"_uuid":"44940abc0763a58db09eff112338ed06ee23d03a"},"source":"traintest=train.append(test)\ntraintest=traintest.merge(right=severity, on='id')\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"69915d28-3fc2-4675-a310-527645e877c9","collapsed":true,"_uuid":"95d5a7a0af96ff1b6e602b8f42fbb40148ef0648"},"source":"resource_by_id=pd.get_dummies(resource,columns=['resource_type'])\nresource_by_id=resource_by_id.groupby(['id']).sum().reset_index(drop=False)\n\nevent_by_id=pd.get_dummies(event,columns=['event_type'])\nevent_by_id=event_by_id.groupby(['id']).sum().reset_index(drop=False)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"resource_by_id.head(5)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"bca27478-5fc5-4108-a3ef-f1f4f056654c","_uuid":"5da42465de95839f0494bc3e50516b30e943e524"},"source":"For log feature data, this is slightly more complex, as we also need to capture the volume value associate to each id & log feature pair. So what we do here is for each id create columens for all the log feature with defaul value to be 0, and use the volume value if the corresponding pair of id and log feature is present in the log feature file.  \n\nWe do this by firstly construct a dictionary to hold all the id, log feature and volume information. And then we construct a numpy array with the shape to accomendate id and all the log feature columns - one unique id per row, and finally we fill values from the dictionary into the array accordingly to construct the resulted dataframe","cell_type":"markdown"},{"metadata":{"_cell_guid":"4ed45992-d379-4090-bef5-f13251e8ed10","collapsed":true,"_uuid":"41dd265649b5df5992d95b34bb276efcda46ce73"},"source":"log_feature_dict={}\n\nfor row in log_feature.itertuples():\n    if row.id not in log_feature_dict:\n        log_feature_dict[row.id]={}\n    if row.log_feature not in log_feature_dict[row.id]:\n        log_feature_dict[row.id][row.log_feature]=row.volume\n\ncolnames=['id']\nfor i in range(1,387):\n    colnames.append('log_feature_'+str(i))\n\nlog_feature_by_id_np=np.zeros((18552,387))\ncount=0\nfor key, feature_dict in log_feature_dict.items():\n    log_feature_by_id_np[count, 0]=np.int(key)\n    for feature, volume in feature_dict.items():\n        log_feature_by_id_np[count, feature]=np.int(volume)\n    count+=1\nlog_feature_by_id=pd.DataFrame(data=log_feature_by_id_np, columns=colnames, dtype=np.int)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"log_feature_by_id.head(3)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5b6598be-2d9c-4067-9b6e-fdf525af2dbb","_uuid":"0bbdbc9990bb8c4181437c916016668d360c3229"},"source":"let's examine the resulted dataframes with the reshaped resource, event and log feature data ","cell_type":"markdown"},{"metadata":{"_cell_guid":"bbabc488-cae3-4a21-8eb4-ff9bed8c46ae","_uuid":"c164e28d2516fc4ada8cba94c298bf1de652354b"},"source":"print(traintest.shape)\nprint(resource_by_id.shape)\nprint(event_by_id.shape)\nprint(log_feature_by_id.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9ebf1924-20ad-4f4b-8cf5-ea9cdbf7bfbe","_uuid":"0866a02cf7477ba57ca6974fed2ab93ca761047a"},"source":"now we are ready to join the data up","cell_type":"markdown"},{"metadata":{"_cell_guid":"495e3911-f8bb-473a-bb46-8c09bc5e90df","_uuid":"1fbcf4218f944d83f054041b55eb091a9069bdb3"},"source":"traintest=traintest.merge(right=severity, on='id')\nprint(traintest.shape)\n\ntraintest=traintest.merge(right=resource_by_id, on='id')\nprint(traintest.shape)\n\ntraintest=traintest.merge(right=event_by_id, on='id')\nprint(traintest.shape)\n\ntraintest=traintest.merge(right=log_feature_by_id, on='id')\nprint(traintest.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"36b4ed59-b44f-443e-9dff-7d57c42907bf","_uuid":"b74d60d84cac7de8b74d02022dd678918cbfaa20"},"source":"final step! we split the data back into train and test input data for machine learning ","cell_type":"markdown"},{"metadata":{"_cell_guid":"df9163da-45be-42a2-b698-02f9082f5e88","collapsed":true,"_uuid":"9a13659f474a30d3e08e6dc9d3e24bffbec3b58c"},"source":"train_input=traintest.loc[0:train.shape[0]-1]\nprint(\"train_input shape is\", train_input.shape)\n\ntest_input=traintest.loc[train.shape[0]::]\nprint(\"test_inpue shape is\", test_input.shape)","execution_count":null,"cell_type":"code","outputs":[]}]}