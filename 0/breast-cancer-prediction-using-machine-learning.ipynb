{"cells":[{"source":"## Prediction of Breast Cancer using SVM with 99% accuracy","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"944c75ecf0178c6158c015537796631f4be3ba67","_cell_guid":"7dc2f0f0-7f2e-441b-ad5e-dc6c6e7e19b5"}},{"source":"Using the Breast Cancer Wisconsin (Diagnostic) Database, we can create a classifier that can help diagnose patients and predict the likelihood of a breast cancer. A few machine learning techniques will be explored. In this exercise, Support Vector Machine is being implemented with 99% accuracy.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"3ffdd62aeaec4f87cefa896ee8f46fba8ac12876","_cell_guid":"e06041ba-c4de-404d-b427-67773487cebb"}},{"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport time","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"44a9940e9fc2880db72ff036201617f1e2dcc2ac","_execution_state":"idle","_cell_guid":"5c7e9232-776b-4250-bc66-846d0211a8d6","trusted":false,"collapsed":true}},{"source":"## Exploratory analysis\n\nLoad the dataset and do some quick exploratory analysis.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"9de448d38725cef68acd836b44c1f6cae2a280a4","_cell_guid":"10a5454a-2edb-4b71-95ce-cf3eb42ad08d"}},{"source":"data = pd.read_csv('../input/data.csv', index_col=False)\ndata.head(5)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"7175ebe2ad5a61144de6985dd44398f0c7778a76","_execution_state":"idle","_cell_guid":"991fcaa2-e4b6-4d1e-a480-c5335249546a","trusted":false}},{"source":"print(data.shape)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"769147a9ae9ba27e572d07c2e75aa9b13287ee9b","_cell_guid":"0f5a9ef3-2b7e-40f3-94c9-d610c0f88dc5","trusted":false}},{"source":"data.describe()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"54c91f25e50f6b069ed7217c34df68cd52cd5970","_cell_guid":"dfdb2cf8-9fe1-4efb-a779-9cb062a04c71","trusted":false}},{"source":"## Data visualisation and pre-processing\n\nFirst thing to do is to enumerate the diagnosis column such that M = 1, B = 0. Then, I set the ID column to be the index of the dataframe. Afterall, the ID column will not be used for machine learning","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"24c05ec20cf1358ff6fda1a37e338f020017caf8","_cell_guid":"dd03fdb7-3144-4150-92de-a3f2132f233d"}},{"source":"data['diagnosis'] = data['diagnosis'].apply(lambda x: '1' if x == 'M' else '0')\ndata = data.set_index('id')\ndel data['Unnamed: 32']","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"7bea9ce154dbe82c5d29588f2e27f00bfe3fdf1d","_execution_state":"idle","_cell_guid":"8ab2f1c6-b174-4a0a-bf9f-7c94326b3e61","trusted":false,"collapsed":true}},{"source":"Let's take a look at the number of Benign and Maglinant cases from the dataset. From the output shown below, majority of the cases are benign (0).","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"81ac236c6c3c0e8206956433426a02779479c094","_cell_guid":"14412b26-d247-41ee-880f-f361a88e4110"}},{"source":"print(data.groupby('diagnosis').size())","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"1ec0c60db5f383231016f03dd8e8ff362b08dbc6","_execution_state":"idle","_cell_guid":"f5936456-74bf-4c42-824f-1973566313c3","trusted":false}},{"source":"Next, we visualise the data using density plots to get a sense of the data distribution. From the outputs below, you can see the data shows a general gaussian distribution. ","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"6baf17aab6d5183b80e7f3a9aadaca3dded6c3b6","_cell_guid":"3c027c06-7285-4bd4-b2c4-ad21e1e293da"}},{"source":"data.plot(kind='density', subplots=True, layout=(5,7), sharex=False, legend=False, fontsize=1)\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"40e65d54d983261df308a4aa138b649e4b86725a","_execution_state":"idle","_cell_guid":"b5457977-bd86-4a6f-87da-8d1acc361852","trusted":false}},{"source":"It is good to check the correlations between the attributes. From the output graph below, The red around\nthe diagonal suggests that attributes are correlated with each other. The yellow and green patches suggest some moderate correlation and the blue boxes show negative correlations. ","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"61fd93395d08de8251226f954320b8cbf4c02871","_cell_guid":"8b3bc827-7651-4be4-800a-e16727af83bc"}},{"source":"from matplotlib import cm as cm\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\ncmap = cm.get_cmap('jet', 30)\ncax = ax1.imshow(data.corr(), interpolation=\"none\", cmap=cmap)\nax1.grid(True)\nplt.title('Breast Cancer Attributes Correlation')\n# Add colorbar, make sure to specify tick locations to match desired ticklabels\nfig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"30721fd7c6e1b4bc1b4c711db44a094969a92b1e","_execution_state":"idle","_cell_guid":"7025c069-528a-4f68-902d-6964ccbb2eb5","trusted":false}},{"source":"Finally, we'll split the data into predictor variables and target variable, following by breaking them into train and test sets. We will use 20% of the data as test set.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"dacd3192e8f4b70e41d714beea8c537a1d5faac6","_cell_guid":"80f7e0c1-4e55-4803-bf95-e16e41994a07"}},{"source":"Y = data['diagnosis'].values\nX = data.drop('diagnosis', axis=1).values\n\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.20, random_state=21)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"770c88e9d8f8f5f0744046d593942f6d7efe1ec5","_cell_guid":"e5f9262f-40c7-4935-9e72-f9be15e113e1","trusted":false,"collapsed":true}},{"source":"## Baseline algorithm checking\n\nFrom the dataset, we will analysis and build a model to predict if a given set of symptoms lead to breast cancer. This is a binary classification problem, and a few algorithms are appropriate for use. Since we do not know which one will perform the best at the point, we will do a quick test on the few appropriate algorithms with default setting to get an early indication of how each of them perform. We will use 10 fold cross validation for each testing.\n\nThe following non-linear algorithms will be used, namely: **Classification and Regression Trees (CART)**, **Linear Support Vector Machines (SVM)**, **Gaussian Naive Bayes (NB)** and **k-Nearest Neighbors (KNN)**.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"e85ea911c9057cda526ed82e742c1fa3daff5746","_cell_guid":"6eb9751e-a4e4-457e-b0e3-728334b80999"}},{"source":"models_list = []\nmodels_list.append(('CART', DecisionTreeClassifier()))\nmodels_list.append(('SVM', SVC())) \nmodels_list.append(('NB', GaussianNB()))\nmodels_list.append(('KNN', KNeighborsClassifier()))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"1b6ac44446c91123c1ceb2bd027e60ab5eb35b9c","_cell_guid":"9d75c60c-5a84-4cf1-abc4-83ccf1a04ea5","trusted":false,"collapsed":true}},{"source":"num_folds = 10\nresults = []\nnames = []\n\nfor name, model in models_list:\n    kfold = KFold(n_splits=num_folds, random_state=123)\n    start = time.time()\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    end = time.time()\n    results.append(cv_results)\n    names.append(name)\n    print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"971d250d87e73a7f30cc137e4625c30d8c5331c8","_cell_guid":"b3919d00-b7aa-4d62-b438-ee822ed6e50e","trusted":false}},{"source":"fig = plt.figure()\nfig.suptitle('Performance Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"cc6a875d67a7d600f983e565332843fa98254f9e","_cell_guid":"04fa9fd2-636c-4f94-94b8-1c9230c85a5a","trusted":false}},{"source":"From the initial run, it looks like GaussianNB, KNN and CART performed the best given the dataset (all above 92% mean accuracy). Support Vector Machine has a surprisingly bad performance here. However, if we standardise the input dataset, it's performance should improve.  ","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"894f687f2f616f40217321a2690ff36d0789ca50","_cell_guid":"c5f330ec-666a-4ee6-85c8-bd06389005d9"}},{"source":"## Evaluation of algorithm on Standardised Data\n\nThe performance of the few machine learning algorithm could be improved if a standardised dataset is being used. The improvement is likely for all the models. I will use pipelines that standardize the data and build the model for each fold in the cross-validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"c2e22d7495f32f54e7a6f9971503b38a71a210f4","_cell_guid":"71509d84-7641-4321-a7d2-60f77deb1e4a"}},{"source":"import warnings\n\n# Standardize the dataset\npipelines = []\n\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\n                                                                        DecisionTreeClassifier())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC( ))])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\n                                                                      GaussianNB())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\n                                                                       KNeighborsClassifier())])))\nresults = []\nnames = []\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    kfold = KFold(n_splits=num_folds, random_state=123)\n    for name, model in pipelines:\n        start = time.time()\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n        end = time.time()\n        results.append(cv_results)\n        names.append(name)\n        print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"2c785e3b14836708690209ac1249220ea98c2592","_cell_guid":"d8b48313-697e-46cd-8a24-c862912e77cd","trusted":false}},{"source":"fig = plt.figure()\nfig.suptitle('Performance Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"09c717fbeeaf8622c7328733812f93f0f1183a43","_cell_guid":"5595bdce-4044-4a29-93ba-3003efdbc4fe","trusted":false}},{"source":"Notice the drastic improvement of SVM after using scaled data. \n\nNext, we'll fine tune the performance of SVM by tuning the algorithm\n\n## Algorithm Tuning - Tuning SVM\n\nWe will focus on SVM for the algorithm tuning. We can tune **two** key parameter of the SVM algorithm - the value of C and the type of kernel. The default C for SVM is 1.0 and the kernel is Radial Basis Function (RBF). We will use the grid search method using 10-fold cross-validation with a standardized copy of the sample training dataset. We will try over a combination of C values and the following kernel types 'linear', 'poly', 'rbf' and 'sigmoid","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"6f63b31910e59133a0d05d2fa88f93b14092cdd1","_cell_guid":"10287c3c-59a8-492a-aead-13bef7ba71d1"}},{"source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=21)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"68186b006294aa46d3cc6f9858445fc009c3e7c0","_cell_guid":"57e17e0e-7eb1-493b-9e12-da601aeaca30","trusted":false}},{"source":"We can see the most accurate configuration was SVM with an **RBF** kernel and **C=1.5**, with the accuracy of **96.92%**.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"8a4634f59a3d171ef5d48a67d7961a7ffd8134ba","_cell_guid":"094e29a4-0b77-4578-995d-7aa600e4fcb8"}},{"source":"## Application of SVC on dataset\n\nLet's fit the SVM to the dataset and see how it performs given the test data.","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"8975c42f962cc0bde2f8ed8e763b021110e34b8f","_cell_guid":"b59e3fd3-9b58-4ec9-8129-bd43dfbecd13"}},{"source":"# prepare the model\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    scaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nmodel = SVC(C=2.0, kernel='rbf')\nstart = time.time()\nmodel.fit(X_train_scaled, Y_train)\nend = time.time()\nprint( \"Run Time: %f\" % (end-start))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"56cdb1896c4c52b05e655485efbc433a770cebd6","_cell_guid":"d22dbf43-8e52-45be-90f1-e93239d624e2","trusted":false}},{"source":"# estimate accuracy on test dataset\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    X_test_scaled = scaler.transform(X_test)\npredictions = model.predict(X_test_scaled)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"7ee346d6b8f2a86c8b51b8237dceed178737ca4c","_cell_guid":"062dac6f-e9dd-45f3-bbce-ef55d921b43f","trusted":false,"collapsed":true}},{"source":"print(\"Accuracy score %f\" % accuracy_score(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"fafb1fdc0bd04148e85c9e069aff39c78e06ddc8","_cell_guid":"334688fe-9900-4872-a0f9-ab2758964d89","trusted":false}},{"source":"print(confusion_matrix(Y_test, predictions))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"73b0c7c8bf76432b2d98b8978e031397d0055c55","_cell_guid":"158e4ad0-e901-4769-bc61-b9cdc310813f","trusted":false}},{"source":"We can see that we achieve an accuracy of 99.1% on the held-out test dataset. From the confusion matrix, there is only 1 case of mis-classification. The performance of this algorithm is expected to be high given the symptoms for breast cancer should exchibit certain clear patterns. ","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"e4d054eadfabe28710fe9eb5e49207c1ae2e6969","_cell_guid":"93e258f1-8725-4f6b-9ee5-7b360cc77952"}},{"source":"## What else could be done\n\n1. Test the algorithm on KNN and GausianNB\n2. Test the data with Artificial Neural Net ","execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"21e532367c3d9ad464471ecdb51df70f839ee0b7","_cell_guid":"711380de-5eea-43ca-8aa0-0ea788c3593a"}},{"source":"","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"2c6792f4b693836e7af04656b0b9bb70c3384705","_cell_guid":"c6c26a28-063c-4efb-916f-81e5f1eaa202","trusted":false,"collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"coursera":{"graded_item_id":"f9SY5","part_id":"mh1Vo","launcher_item_id":"oxndk","course_slug":"python-machine-learning"}}}