{"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"outputs":[],"metadata":{"_uuid":"bd06de28c66d319f23fcd76b331f30c1ef2869e6","_cell_guid":"2b74a927-46a8-4c75-9aba-11287bed56b8","trusted":true},"cell_type":"code","execution_count":null,"source":"import h5py\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\nfrom keras.utils.io_utils import HDF5Matrix\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Activation\nbase_path = os.path.join('..', 'input')\ntrain_h5_path = os.path.join(base_path, 'food_c101_n10099_r32x32x1.h5')\ntest_h5_path = os.path.join(base_path, 'food_test_c101_n1000_r32x32x1.h5')\n%matplotlib inline"},{"outputs":[],"metadata":{"_uuid":"d855f62d485261518e76c36497f22b06eed3f57d","_cell_guid":"64b3ad08-3b26-4e9b-8bc4-df39f85d744c"},"cell_type":"markdown","execution_count":null,"source":"# Load the training and validation data\nWe use the HDF5Matrix class to make loading the data easier"},{"outputs":[],"metadata":{"_uuid":"b4302a8d9eeaa8185aefe48715dff25b282e9db8","_cell_guid":"b14e2f51-2a11-4977-b85c-7d9b81f990a9","trusted":true},"cell_type":"code","execution_count":null,"source":"X_train = HDF5Matrix(train_h5_path, 'images')\ny_train = HDF5Matrix(train_h5_path, 'category')\nprint('In Data',X_train.shape,'=>', y_train.shape)"},{"outputs":[],"metadata":{"_uuid":"452d61f62ee5f040efddad647fe2feb90e97666f","_cell_guid":"14b03b92-e658-4012-a6ef-12e28b26bd2b","trusted":true},"cell_type":"code","execution_count":null,"source":"X_test = HDF5Matrix(test_h5_path, 'images')\ny_test = HDF5Matrix(test_h5_path, 'category')\nprint('In Data',X_test.shape,'=>', y_test.shape)"},{"outputs":[],"metadata":{"trusted":true,"_uuid":"a1d00afc603b29fb96d614588c4d692abbf31689"},"cell_type":"code","execution_count":null,"source":"sample_imgs = 25\nwith h5py.File(train_h5_path, 'r') as n_file:\n    total_imgs = n_file['images'].shape[0]\n    read_idxs = slice(0,sample_imgs)\n    im_data = n_file['images'][read_idxs]\n    im_label = n_file['category'].value[read_idxs]\n    label_names = [x.decode() for x in n_file['category_names'].value]\nfig, m_ax = plt.subplots(5, 5, figsize = (12, 12))\nfor c_ax, c_label, c_img in zip(m_ax.flatten(), im_label, im_data):\n    c_ax.imshow(c_img if c_img.shape[2]==3 else c_img[:,:,0], cmap = 'gray')\n    c_ax.axis('off')\n    c_ax.set_title(label_names[np.argmax(c_label)])"},{"outputs":[],"metadata":{"_uuid":"a23c28f0c3f82ac65d3e9a2828ffcc9c17c511d4"},"cell_type":"markdown","execution_count":null,"source":"# Building the network\nWe build a very basic neural network for classifying the images, based on a simple Keras demo script. The network is quick to train and well suited for small datasets like the one we are using. Dropout layers have been added to prevent overfitting"},{"outputs":[],"metadata":{"_uuid":"3448ece339e64b057d645f21075a87551bf05872","_cell_guid":"862fbcf5-8a19-40b2-b57b-bc254b55d6f3","trusted":true},"cell_type":"code","execution_count":null,"source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_test.shape[1]))\nmodel.add(Activation('softmax'))\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nloss_history = []\nmodel.summary()"},{"outputs":[],"metadata":{"_uuid":"b5ca068165dffdcf18a41d3c501e2a05426c47e7","_cell_guid":"81bbd2c3-f7fe-4a2f-99df-90cf6492cb85","trusted":true},"cell_type":"code","execution_count":null,"source":"for i in range(10):\n    loss_history += [model.fit(X_train, y_train,\n                               validation_data=(X_test, y_test), \n                               batch_size = 256,\n                               epochs = 1, shuffle=\"batch\")]"},{"outputs":[],"metadata":{"_uuid":"89e8c15d754e3f3c7e7313b114f7b42f15ee4d40"},"cell_type":"markdown","execution_count":null,"source":"# Examining Loss\nThe following function shows the loss and accuracy over the course of training. We see the training data curves in blue and the validation in red"},{"outputs":[],"metadata":{"_uuid":"5992a55d8eb69109396e8bd1183b97e86c1a40e8","_cell_guid":"5e1e564a-a145-4969-b7c8-c9926104d78e","trusted":true},"cell_type":"code","execution_count":null,"source":"epich = np.cumsum(np.concatenate(\n    [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n_ = ax1.plot(epich,\n             np.concatenate([mh.history['loss'] for mh in loss_history]),\n             'b-',\n             epich, np.concatenate(\n        [mh.history['val_loss'] for mh in loss_history]), 'r-')\nax1.legend(['Training', 'Validation'])\nax1.set_title('Loss')\n\n_ = ax2.plot(epich, np.concatenate(\n    [mh.history['acc'] for mh in loss_history]), 'b-',\n                 epich, np.concatenate(\n        [mh.history['val_acc'] for mh in loss_history]),\n                 'r-')\nax2.legend(['Training', 'Validation'])\nax2.set_title('Accuracy')"},{"outputs":[],"metadata":{"_uuid":"0acdf7ba8126cb1d54c4c70029c50100069b364c"},"cell_type":"markdown","execution_count":null,"source":"# Visualizing Results\nHere we show the results a small sample of validation images to see how well the predictions match the data"},{"outputs":[],"metadata":{"_uuid":"58bdcedc32efb3a249b71e54047688229c9eed17","_cell_guid":"c449e416-2df3-4460-8e8a-aff65d0839f5","trusted":true},"cell_type":"code","execution_count":null,"source":"sample_imgs = 16\nwith h5py.File(test_h5_path, 'r') as n_file:\n    total_imgs = n_file['images'].shape[0]\n    read_idxs = slice(0,sample_imgs)\n    im_data = n_file['images'][read_idxs]\n    im_label = n_file['category'].value[read_idxs]\n    label_names = [x.decode() for x in n_file['category_names'].value]\npred_label = model.predict(im_data)\nfig, m_ax = plt.subplots(4, 4, figsize = (20, 20))\nfor c_ax, c_label, c_pred, c_img in zip(m_ax.flatten(), im_label, pred_label, im_data):\n    c_ax.imshow(c_img if c_img.shape[2]==3 else c_img[:,:,0], cmap = 'gray')\n    c_ax.axis('off')\n    c_ax.set_title('Predicted:{}\\nActual:{}'.format(label_names[np.argmax(c_pred)],\n                                                  label_names[np.argmax(c_label)]))"},{"outputs":[],"metadata":{"collapsed":true,"trusted":true,"_uuid":"32f6617b2218605eacff7b3757d8a6840782dcbb"},"cell_type":"code","execution_count":null,"source":""}],"nbformat":4}