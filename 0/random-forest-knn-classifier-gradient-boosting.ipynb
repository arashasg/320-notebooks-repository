{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop the unneccasary Varaibles- ID, Unnamed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets we map the target varaible and have a look of those values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis']=df['diagnosis'].map({'B':0,'M':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"diagnosis\"].value_counts(normalize=True).plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we proceed we have a visualisation about the feature and its influence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation plot(Heat Map)--only values the relation\n# Lets use pair plot to get an overall idea about the data for comparision\n#sns.pairplot(df,hue=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"plt.figure(figsize=(10,10))\nsns.boxplot(x= df.columns[-2:], y=df[\"diagnosis\"])\nplt.xticks(rotation=90)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a X and y ready for the classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= df.drop(\"diagnosis\",axis=1)\ny= df[\"diagnosis\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Taken from kaggle as a reference, where we use standardization of the variable and have a look of the distribution\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest for prediction\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\nrfc= RandomForestClassifier(n_estimators=10)\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the train on the model\ny_train_pred =rfc.predict(X_train)\ny_train_prob =rfc.predict_proba(X_train)[:,1]\n\nprint(\"Accuracy Score of train\", accuracy_score(y_train,y_train_pred))\nprint(\"AUC of the train \", roc_auc_score(y_train,y_train_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_train,y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model on Test data \ny_test_pred =rfc.predict(X_test)\ny_test_prob =rfc.predict_proba(X_test)[:,1]\n\nprint(\"Accuracy Score of test\", accuracy_score(y_test,y_test_pred))\nprint(\"AUC od the test \", roc_auc_score(y_test,y_test_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_test,y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since KNN is a distance based Algorithm- we need to do standardization of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3) \nclf.fit(X_train, y_train)  \nprint(clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the train on the model\ny_train_pred =clf.predict(X_train)\ny_train_prob =clf.predict_proba(X_train)[:,1]\n\nprint(\"Accuracy Score of train\", accuracy_score(y_train,y_train_pred))\nprint(\"AUC of the train \", roc_auc_score(y_train,y_train_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_train,y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model on Test data \ny_test_pred =clf.predict(X_test)\ny_test_prob =clf.predict_proba(X_test)[:,1]\n\nprint(\"Accuracy Score of test\", accuracy_score(y_test,y_test_pred))\nprint(\"AUC od the test \", roc_auc_score(y_test,y_test_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_test,y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nlr_list= [0.05,0.075,0.1,0.25,0.5,0.75,1] #Learning Rate\n\nfor learning_rate in lr_list:\n    clf= GradientBoostingClassifier(n_estimators=20, \n                                    learning_rate=learning_rate,max_features=2,max_depth=2,random_state=0)\n    clf.fit(X_train,y_train)\n\n    print(\"Learning Rate :\", learning_rate)\n    print(\" Accuracy rate of training \", clf.score(X_train,y_train))\n    print(\"Accuracy score of the test :\", clf.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best Accuracy rate is been observed in the Learning Rate : 0.5\n# Accuracy rate of training  1.0\n# Accuracy score of the test : 0.9415204678362573","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_new= GradientBoostingClassifier(n_estimators=10,max_features=2,learning_rate=5,random_state=1)\nclf_new.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the train on the model\ny_train_pred =clf_new.predict(X_train)\ny_train_prob =clf_new.predict_proba(X_train)[:,1]\n\nprint(\"Accuracy Score of train\", accuracy_score(y_train,y_train_pred))\nprint(\"AUC of the train \", roc_auc_score(y_train,y_train_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_train,y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model on Test data \ny_test_pred =clf_new.predict(X_test)\ny_test_prob =clf_new.predict_proba(X_test)[:,1]\n\nprint(\"Accuracy Score of test\", accuracy_score(y_test,y_test_pred))\nprint(\"AUC od the test \", roc_auc_score(y_test,y_test_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_test,y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall Inference:\n      - KNN Classifier using the concept of the distance algorithm performed the best in determining the AUC(Area Under the curve as it been more valid in the classifier compared to the accuracy in classifiers)\n      - Random Forest Classfier(with 10 estimators) provided with the comparable auc score of 97%\n      - Gradient Descent (Using the leraning rate to minimise the error rate) provided with the accuracy of 88%. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}