{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Some Usefull libraris","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.gridspec import GridSpec\n\n#Plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n#Some styling\nsns.set_style(\"darkgrid\")\nplt.style.use(\"fivethirtyeight\")\n\n#Subplots\nfrom plotly.subplots import make_subplots\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing House prices data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **SalePrice** is our target variable. We have to predict the best price of house","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data :>',train_data.shape)\nprint('-'*123)\nprint('Test Data :>',test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data :>',train_data.info())\nprint('-' * 123)\nprint('Test Data :>',test_data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above information tells us\n\n* Our dataset features consists of three datatypes\n    1. float\n    2. integer\n    3. object\n* Of which total numerical features are 38\n* And categorical features are 43.\n* But if we look closely , we see that some of the data types are incorrect.\n * For ex :- MSSubClass,OverallQual and OverallCond should be object data types.\n* Also we don't have complete data for all of our features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Training Data \n* There are total 1460 observation with 81 columns/variable/features.\n* There are both numerical and categorical data.\n\n### Test Data\n* There are total 1459 observation with 80 columns/variable/features.\n* The missing variable is **SalePrice** as it is the target column that we want to predict.\n* There are both numerical and categorical data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive measure of data\ntrain_data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This doesn't help us much, let's try to visualize the number of missing values in each feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we create a list of missing values by each feature \ntemp = list(train_data.isna().sum())\n# them we create a list of columns and their missing values as inner list to a separate list\nlst=[]\ni=0\nfor col in train_data.columns:\n    insert_lst = [col,temp[i]]\n    lst.append(insert_lst)\n    i+=1\n\n# finally create a dataframe\ntemp_train_data = pd.DataFrame(data=lst,columns=['Column_Name','Missing_Values'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(temp_train_data.sort_values(by='Missing_Values'),x='Missing_Values',y='Column_Name',\n             orientation='h',height=1500,width=900,color='Missing_Values',text='Missing_Values')\nfig.update_traces(textposition='outside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following columns have missing values\ntemp_train_data[temp_train_data['Missing_Values']>0].sort_values(by='Missing_Values',\n                                                 ascending=False).reset_index(drop=True).style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Out of the 18 columns with missing values,\n* Three are numerical features LotFrontage,MasVnrArea and GarageYrBlt\n* And the rest are categorical features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Finding the missing value of train data\n* Removing the missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_name = []\nfor i in train_data.columns:\n    if train_data[i].isna().sum()/len(train_data)*100 >=10:\n        columns_name.append(i)\nprint(columns_name)# List of columns which has >=10% missing data      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping columns in train data\ntrain_data = train_data.drop(columns=columns_name,axis = 1)\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Skewness**\n* Describe how are distributed\n* It is measure of shape : Symmetrical or Asymmetrical","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Target Feature\n* SalePrice","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.SalePrice.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our target feature is a continuous variable with values ranging from 34900 to 755000.\n* The average sale price of all the houses in our dataset is 180921.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1,cols=2)\n\nfig.add_trace(go.Histogram(x=train_data['SalePrice']),row=1,col=1)\nfig.add_trace(go.Box(y=train_data['SalePrice'],boxpoints='all',line_color='orange'),row=1,col=2)\n\nfig.update_layout(height=500, showlegend=False,title_text='SalePrice Distribution and Box Plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sale Price has a right skewed distribution.\n* The median sale price of our dataset is 163000 which is less than the average value i.e because of right skewed distribution.\n* We can see some of the houses have sale price more than 4,00,000.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the skewness\ntrain_data.skew().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Skewness tells us about the symmetry in a distribution.\n* If Skewness is equal to zero , It is a symmetrical distribution.\n* And If Skewness is less than or more than zero then it is a non-symmetrical distribution.\n* If value is less than zero , distribution is left skewed and value is more than zero , distribution is right skewed.\n* In our above data,\n    1. LotArea\n    2. LowQualFinSF\n    3. SsnPorchPoolArea\n    4. MiscVal\n* Are highly positively,right skewed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **BsmtUnfSF, 2ndFlrSF, OverallCond, TotRmsAbvGrd, HalfBath, Fireplaces, BsmtFullBath, OverallQual, MoSold, BedroomAbvGr, GarageArea, YrSold, FullBath, Id, GarageCars, YearRemodAdd, YearBuilt, GarageYrBlt** Variables are nearly Symmetical in shape\n\n\n### Outliers \n* **MiscVal, PoolArea, LotArea, 3SsnPorch, LowQualFinSF, KitchenAbvGr, BsmtFinSF2, ScreenPorch, BsmtHalfBath, EnclosedPorch, MasVnrArea, OpenPorchSF, LotFrontage, SalePrice, BsmtFinSF1, WoodDeckSF, TotalBsmtSF, MSSubClass, 1stFlrSF, GrLivArea** Variables are positively skewed (asymmetrical in shape)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation between different features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(100,90))\nsns.heatmap(train_data.corr(),annot = True,fmt=\".1f\",annot_kws={'size':48})\n# Returns correlation among fatures which obervations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(train_data, x=\"SalePrice\", color='OverallQual',barmode=\"overlay\",title=\"Overall Quality of the house\")\nfig.update_layout(height=500)\nfig.show()\n\nfig = px.histogram(train_data, x=\"SalePrice\", color='OverallCond',barmode=\"overlay\",title=\"Overall Condition of the house\")\nfig.update_layout(height=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### creating feature data and target data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_data = train_data.drop(columns=['SalePrice'],axis=1)\ntarget_data =train_data.SalePrice ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# int and Float columns\nint_float_data = feature_data.select_dtypes(include=['int','float'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical columns\ncat_data = feature_data.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Pipeline model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating sub-pipeline \nfloat_int_pipeline = make_pipeline(SimpleImputer(strategy='median'),MinMaxScaler())\ncat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),OrdinalEncoder())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming columns (preprocessing)\npreprocessor = make_column_transformer(\n    (float_int_pipeline,int_float_data.columns),\n    (cat_pipeline,cat_data.columns)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using LinearRegression\npipeline = make_pipeline(preprocessor, LinearRegression())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX, testX, trainY, testY = train_test_split(feature_data, target_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(feature_data, target_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.score(testX,testY) # using LinearRegression wwe geting 0.69","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now using RandomForestClassifier\npipeline = make_pipeline(preprocessor, RandomForestClassifier()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(feature_data, target_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.score(testX,testY) # For this data set the RandomForestClassifier is best model to predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now check the mean_squared_error\ny_pred = pipeline.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(testY, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(testY, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipeline.predict(test_data)\nsubmission = pd.DataFrame({'Id': test_data.index,'SalePrice': y_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"house_prices_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}