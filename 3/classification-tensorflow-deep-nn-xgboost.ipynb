{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telco customer churn - binary classification problem\nAncient problem of machine learning - will customer churn or not? Let's do some analysis, preprocessing, feature engineering and then apply XGB Classifier & Tensorflow on our data to predict churn."},{"metadata":{},"cell_type":"markdown","source":"![](https://osclasspoint.com/images/customer-churn.png)"},{"metadata":{},"cell_type":"markdown","source":"# Load libraries\nNothing extraordinary will be used - numpy, pandas, sklearn, matplotlib, seaborn, xgboost and tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import resample\n\n%matplotlib inline\npd.options.display.max_columns = 500\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data\nLoad data using pandas, we have just one dataset here that makes things easier"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore our data\nFirst chech top rows, then columns format and missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we have no null values, that is great and save us some time, however TotalCharges seems to have incorrect format (object), fix this converting field to float (float64) and filling missing values those will be generated during conversion with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(value=0)\n\ndf['tenure'] = df['tenure'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop customer ID as it's not relevant field for analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('customerID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split our fields to categorical and numerical so we can do EDA & preprocessing faster. Churn, our target variable, will not be included in categorical fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_cat = df.select_dtypes(include='object').drop('Churn', axis=1).columns.tolist()\ncol_num = df.select_dtypes(exclude='object').columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"For our categorical fields, check how many unique values has each column so we will decide if feature engineering (and merging values in case there is too many of them) is needed.\nYou will see we have 2-4 unique values that is ideal."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in col_cat:\n    print('Column {} unique values: {}'.format(c, len(df[c].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look on distribution of Churn across all categorical variables. This is really nice view where you can see that i.e. gender is not correlated with Churn at all, but Contract is highly correlated with churn and customers having month-to-month contract are much more likely to churn, comparing to customers with 1-year and 2-years contracts. That's intresting fact and can help company to make 1 & 2 years contract more attractive!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,c in enumerate(col_cat):\n    plt.subplot(5,4,i+1)\n    sns.countplot(df[c], hue=df['Churn'])\n    plt.title(c)\n    plt.xlabel('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checkout distribution of our numerical features. We again want to find out some interesting relations in data.\nIt seems tenure is correlated with Churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n    plt.subplot(1,3,i+1)\n    sns.distplot(df[df['Churn'] == 'No'][c], kde=True, color='blue', hist=False, kde_kws=dict(linewidth=2), label='No')\n    sns.distplot(df[df['Churn'] == 'Yes'][c], kde=True, color='Orange', hist=False, kde_kws=dict(linewidth=2), label='Yes')\n    plt.title(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try also violin plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(col_num):\n    plt.subplot(1,4,i+1)\n    sns.violinplot(x=df['Churn'], y=df[c])\n    plt.title(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing\nWe've completed our quick and simple EDA, it's time to cook our data for taste of machine learning algorithms those like just numerical data, not text data :)"},{"metadata":{},"cell_type":"markdown","source":"First, do one hot encoding of our categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfT = pd.get_dummies(df, columns=col_cat)\ndfT.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now do simple label encoding of our target variable Churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfT['Churn'] = dfT['Churn'].map(lambda x: 1 if x == 'Yes' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balanced or imbalanced?\nCheck if our dataset is balanced or imbalanced and if any action is needed. You will find out that data are highly imbalanced, we will use resample function to upsample minority group."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(dfT['Churn'])\nplt.title('Imbalanced dataset, it seems ratio is 2:5 for Yes:No')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Divide our data into 2 groups, majority (0) and minority (1) and create new dataset by upsampling minority group."},{"metadata":{"trusted":true},"cell_type":"code","source":"minority = dfT[dfT.Churn==1]\nmajority = dfT[dfT.Churn==0]\n\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\ndfT = pd.concat([minority_upsample, majority], axis=0)\ndfT = dfT.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do just quick check how it looked like before balance and after balance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.subplot(1,2,1)\nsns.countplot(df['Churn'])\nplt.title('Imbalanced dataset')\n\nplt.subplot(1,2,2)\nsns.countplot(dfT['Churn'])\nplt.title('Balanced dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to scale!\nML algorithms are sensitive on data that are not normalized to same scale. You might try that deep net (at the end of kernel) will have much lower accuracy when using unscaled data... accuracy can go down even by 10%! I will use robust scaler that can nicely handle outliers, but standard scaler might work well too."},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = RobustScaler()\ndfT['tenure'] = rs.fit_transform(dfT['tenure'].values.reshape(-1,1))\ndfT['MonthlyCharges'] = rs.fit_transform(dfT['MonthlyCharges'].values.reshape(-1,1))\ndfT['TotalCharges'] = rs.fit_transform(dfT['TotalCharges'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Split\nSplit our data into train & test partitions. Train partition will be used to train ML model, test will be used to validate it's performance. 80% goes to train, 20% goes to test. It could be also 70:30 or 60:40."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dfT.drop('Churn', axis=1).values, dfT['Churn'].values, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\nOur first try will be XGBoost. We could try Random Forest or Light GBM, but these will not lead to better results comparing to XGBoost, therefore second choice will be deep neural network consisting of multiple layers."},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\nLet's start with popular XGB Classifier and check it's performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_test_hat_xg = xg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_hat_xg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad! We got really good precission as well as recall and f1 score! Yes you are right, I should try some hyperparameter tunning, but for now let's keep this notebook simple. You may find hyperparameter optimization in other of my kernels ;)"},{"metadata":{},"cell_type":"markdown","source":"## Deep neural networks\nYes it should be fun. Using simple net? No, we will use something more complex... Let's do it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use sequential model with multiple dense & dropout layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.45))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, verbose=1,patience=10, min_lr=0.0000000001)\nearly_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping_cb, reduce_lr])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model is trained, you might see it's overfitting during training, so increasing dropout would solve this problem... yes I've tried it and once I solved problem with overfitting, accuracy on test data was decreased :) ...ok, predict our test data and compare it to actuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_hat_tf = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output of prediction are probabilities, let's convert probabilities into 0/1"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_hat_tf2 = [1 if x > 0.5 else 0 for x in y_test_hat_tf ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally checkout classification report!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_hat_tf2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What you think? It seems xgboost is slightly better, but this net was almost catching it ;)"},{"metadata":{},"cell_type":"markdown","source":"That's it, feel free to post your comments ;)"},{"metadata":{},"cell_type":"markdown","source":"## Thanks for checking my notebook, if you liked it, make sure to vote for for this notebook!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}