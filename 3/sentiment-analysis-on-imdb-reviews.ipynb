{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Sentiment Analysis on IMDB Reviews\nThis is a simple project created with the intention of putting into practice knowledge learned about Machine Learning, Natural Language Processing and Sentiment Anaylisis.\n\n## Dataset\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n\nwe do not need to download datasets locally as necessary functions have been included to download dataset from jupyter notebook"},{"metadata":{"id":"t4qpOKjRsWZU","trusted":true},"cell_type":"code","source":"from keras.datasets import imdb","execution_count":null,"outputs":[]},{"metadata":{"id":"7-p-kFd6ILpF"},"cell_type":"markdown","source":"***Data Preparation***"},{"metadata":{"id":"y5g_MHdM_OWv","trusted":true},"cell_type":"code","source":"((XT,YT),(Xt,Yt)) = imdb.load_data(num_words=10000)   #XT training # Xt testing","execution_count":null,"outputs":[]},{"metadata":{"id":"zJpHC0gPIeR0","outputId":"a1f621cd-e9e2-400a-b1e1-3fbab56a1ccb","trusted":true},"cell_type":"code","source":"len(Xt),len(XT)","execution_count":null,"outputs":[]},{"metadata":{"id":"KlXJv5YbIghR","outputId":"8ff350dc-464f-4e63-cae3-5b0f4c7f29e5","trusted":true},"cell_type":"code","source":"print(XT[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"qTw-RhHKIoEE","trusted":true},"cell_type":"code","source":"word_idx = imdb.get_word_index()","execution_count":null,"outputs":[]},{"metadata":{"id":"rTkX_-rYJBg9","outputId":"caa6e223-558f-419c-9941-aa78fdf73dce","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# print(word_idx.items())    # you run this cell to see the output","execution_count":null,"outputs":[]},{"metadata":{"id":"AYYRLO_hJIWs","trusted":true},"cell_type":"code","source":"idx_word = dict([value,key] for (key,value) in word_idx.items())","execution_count":null,"outputs":[]},{"metadata":{"id":"Dey6D1rzKCPY","outputId":"6a7021ac-f006-46d1-b35b-e6d82f1dcfa5","trusted":true},"cell_type":"code","source":"# print(idx_word.items())    # you can also run this cell to see the output","execution_count":null,"outputs":[]},{"metadata":{"id":"H52g3Ag0KaFD","trusted":true},"cell_type":"code","source":"actual_review = ' '.join([idx_word.get(idx-3,'#') for idx in XT[0]])","execution_count":null,"outputs":[]},{"metadata":{"id":"HFZL17kwNvlO","outputId":"a14be73b-b751-41ec-b275-854449058e88","trusted":true},"cell_type":"code","source":"print(actual_review)","execution_count":null,"outputs":[]},{"metadata":{"id":"gl9cjSM9NyoE","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"8OgTF4-yN8Vm","trusted":true},"cell_type":"code","source":"##next step ----> Vectorize the data\n## Vocab size --> 10,000 we will make sure every sentence is represented by a vector of len 10,000 [0000010001001011...]\n\n\ndef  vectorize_sentences(sentences,dim = 10000):\n  outputs = np.zeros((len(sentences),dim))\n\n\n  for i,idx in enumerate(sentences):\n    outputs[i,idx] = 1\n\n  return outputs","execution_count":null,"outputs":[]},{"metadata":{"id":"3eYBZ6fqO9ef","trusted":true},"cell_type":"code","source":"X_train  = vectorize_sentences(XT)\nX_test = vectorize_sentences(Xt)","execution_count":null,"outputs":[]},{"metadata":{"id":"jLc87IQpPial","outputId":"cbca1c5d-3637-4e1b-c54e-be914db63afd","trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"l5GAjf9KQJ4L","outputId":"10a60521-6fdd-4090-ef1b-5038b610349a","trusted":true},"cell_type":"code","source":"print(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"wm2ZvGc2QMeo","trusted":true},"cell_type":"code","source":"Y_train  = np.asarray(YT).astype('float32')\nY_test = np.asarray(Yt).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"id":"bi0xoMRMQs4i"},"cell_type":"markdown","source":"### Build a network\n## Define our model architecture\n1 use fully connected/dense layers with RELU activation\n\n2 two hidden layers with 16 unit each\n\n3 one output layer with 1 unit(sigmoid activation funct)\n"},{"metadata":{"id":"u8dTcY_ZQnkq","trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"id":"i0lMaOMNbJ5h","trusted":true},"cell_type":"code","source":"# define the model\nmodel  = models.Sequential()\nmodel.add(Dense(16,activation = 'relu' , input_shape = (10000,)))\nmodel.add(Dense(16,activation = 'relu'))\nmodel.add(Dense(1,activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"id":"8gMUGNN7emfb","trusted":true},"cell_type":"code","source":"# here we are compiling\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy']) # you can use adam insted of rmsprop","execution_count":null,"outputs":[]},{"metadata":{"id":"L0cCYja_fJ5n","outputId":"1e36dd48-6ec4-404c-a65f-396aa829f8c3","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"3Uem8j8Hf-bR"},"cell_type":"markdown","source":"## Training and validation"},{"metadata":{"id":"M92YxfERgD0l","trusted":true},"cell_type":"code","source":"x_val = X_train[:5000]\nx_train_new = X_train[5000:]\n\ny_val = Y_train[:5000]\ny_train_new = Y_train[5000:]","execution_count":null,"outputs":[]},{"metadata":{"id":"66op8X8qheEf","outputId":"6a55fd4c-d436-4e50-f729-2e6fde056ea7","trusted":true},"cell_type":"code","source":"hist = model.fit(x_train_new,y_train_new,epochs = 4,batch_size=512,validation_data =(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"YsaiQqzEiIaE"},"cell_type":"markdown","source":"## Visualize"},{"metadata":{"id":"hHffbyfgh_1c","trusted":true},"cell_type":"code","source":"h = hist.history","execution_count":null,"outputs":[]},{"metadata":{"id":"fc6XYswBo_v_","outputId":"50a29769-7220-45c2-d8dc-ae8477a72138","trusted":true},"cell_type":"code","source":"plt.plot(h['val_loss'],label = 'validation loss')\nplt.plot(h['loss'],label = 'training loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"id":"quW_D2qDp7H-","outputId":"454e1b35-3b19-4669-d51d-531351be7772","trusted":true},"cell_type":"code","source":"plt.plot(h['val_accuracy'],label = 'validation Acc')\nplt.plot(h['accuracy'],label = 'training Acc')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"id":"LSPIsJGqssDp","trusted":true},"cell_type":"code","source":"h = hist.history","execution_count":null,"outputs":[]},{"metadata":{"id":"x7rSK355t0E6","outputId":"b25f3864-5f6b-4766-e2aa-8c7f01f93b65","trusted":true},"cell_type":"code","source":"# let's calculate accuracy\nmodel.evaluate(X_test,Y_test)[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"6rf1EgR5ucdS","outputId":"a7c35306-bbd1-44c1-d4a1-8e817974b034","trusted":false},"cell_type":"code","source":"model.evaluate(X_train,Y_train)[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"l5gqtIZfvIiG","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}