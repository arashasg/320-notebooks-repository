{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the last column from the df\ndf.drop(columns=['Unnamed: 32'],axis=1,inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**High level overview of the data**\n\nThere are in all 32 features that we are concerned with in the given dataframe. The target column or rather df['diagnosis'] is the diagnosis of breast tissues (M = malignant, B = benign) and the rest 30 features signify the details or rather in independent variables obtained through testing the patients medically which in turn leads to the diagnosis. Column 0 or df['id'] contains the ids of the patients data.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Simple Plot of Area difference between Malignant and Benign Tumors\ndf.groupby('diagnosis').area_mean.mean().plot(kind='bar',color=['r','b'])\nplt.ylabel('Mean Area of Tumor')\nplt.xlabel('Diagnosis of Tumor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 30 seperate data points that are seperate measurements of the tumor and then there is only one target/dependent variable that is the diagnosis , lets use SVC(suport vector classifier) to classify our data. We will be using basic scaling and select our kernel using GridsearchCV.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing our data into X(independent Variables) and Y(target)\nX=df.iloc[:,2:32]\nY=df.iloc[:,1:2]\n#Seperating our data into training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will be using StandardScaler() it is a general purpose scaling tool to scale the data generally used when there is a disparity in our data elements.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Lets use GridsearchCV as we promised to tune our hyperparameters, and then discuss why we did it after getting our result.\n#Defining our classifier\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\nx = [1.0,10.0,100.0,500.0,1000.0]\ny = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nz = [2,3,4]    \nparameters=[{'C': x,'kernel': ['linear']},\n            {'C': x,'kernel': ['rbf'],'gamma': y} ,\n            {'C': x,'kernel': ['poly'],'gamma': y,'degree': z}\n           ]\ngridsearch=GridSearchCV(estimator = classifier,\n                        param_grid = parameters,\n                        scoring='accuracy',\n                        cv=10,\n                        n_jobs=-1)\ngridsearch=gridsearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting our accuracy score for the model\naccuracy=gridsearch.best_score_\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeing our best parameters\ngridsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using the best parameters as suggested by our GridsearchCV to finetune our model and validate it.\nclassifier=SVC(kernel='linear',C=1.0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc=accuracy_score(y_test,y_pred)\nacc=acc*100\nacc=round(acc,2)\nprint('Accuracy Score of our model is: ',acc,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nThis data is clean and dosent have any outliers or missing values. Given this data using the StandardScaler() we scaled our features to negate the possibility of having outliers. Then comes the straineous task of selecting the best hyperparameters for our classifier. We can do this manually by grinding through the hyperparameters and the kernels available to us in the sklearn.svm library under our Support Vector Classifier else we could automate it using GridsearchCV and hence we went for the latter. Using GridsearchCV we discovered that our model performs best with the default *C=1.0 and linear kernel* (anticlimactically as it is the first thing people normally test with XD) however we established that this method of hyperparameter optimization is the best possible as it eases us our work and reduces time many folds specially in cases like these where hyperparameter optimization is of so much importance because of how a error in classification can threaten the patients life.\n\nOur model classifies between Malignant and Benign Tumors with an accuracy of 95.61% and although this is not the best score, this is about the best we can do with the Support Vector Classifier for this particular data. \n\nThanks for reading, have a great day.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}