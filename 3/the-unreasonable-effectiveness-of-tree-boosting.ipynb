{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Unreasonable Effectiveness of Tree Boosting\n\nThis notebook illustrates how gradient boosting can learn even the most complex statistical relationships, at least if you feed enough data to the beast!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports and settings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n%matplotlib inline\nsns.set(font_scale=1.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate data\n\nWe first sample 100'000 data points from the following two-dimensional function:\n$$\n    y = f(x, z) = \\frac{\\sin(x^2 + z^2)}{x^2 + z^2} + 0.5 \\cos(x) - 0.5,\n$$\n$x, z \\in [-2\\pi, 2\\pi]$.\n\nIts profile looks as follows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate data\nn = 100_000\nt = np.linspace(-2 * np.pi, 2 * np.pi, int(np.sqrt(n)))\nx, z = np.meshgrid(t, t)\ny = np.sin(x**2 + z**2) / (x**2 + z**2) + np.cos(x) * 0.5 - 0.5\n\n# Turn to DataFrame\ndata = pd.DataFrame({\n    'x': x.flatten(), \n    'z': z.flatten(),\n    'y': y.flatten()\n})\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 5))\n\n# Response distribution\nsns.distplot(data.y, ax=ax[0])\nax[0].set(title=\"Distribution of y\")\n\n# Response profile\ndef nice_heatmap(data, v, ax):\n    sns.heatmap(data.pivot('z', 'x', v), \n                xticklabels=False, yticklabels=False, \n                cmap='coolwarm', vmin=-1, vmax=1, ax=ax)\n    ax.set_title(\"Heatmap of \" + v)\n    return None\n\nnice_heatmap(data, v='y', ax=ax[1])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test split\n\nIn order to not fall into the trap of overfitting, we set aside 33% of the data lines for testing only. The model is trained on the remaining 67%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    data[[\"x\", \"z\"]], data[\"y\"], \n    test_size=0.33, random_state=63\n)\n\nprint(\"All shapes:\")\nfor dat in (X_train, X_test, y_train, y_test):\n    print(dat.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\n\nNow we fit a gradient boosting tree with [LightGBM](https://github.com/microsoft/LightGBM), besides [XGBoost](https://github.com/dmlc/xgboost) and [CatBoost](https://github.com/catboost/catboost) one of the tree major implementations of gradient boosting. \n\nThe parameters have been manually chosen by five-fold cross-validation on the training data in order to minimize (root-)mean squared error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nparams = {\n    'objective': 'regression',\n    'num_leaves': 63,\n    'metric': 'l2_root',\n    'learning_rate': 0.3,\n    'bagging_fraction': 1,\n    'min_sum_hessian_in_leaf': 0.01\n}\n\n# Data interface\nlgb_train = lgb.Dataset(X_train, label=y_train)\n                 \n# Fitting the model\nif False:\n    # Find good parameter set by cross-validation\n    gbm = lgb.cv(params,\n                 lgb_train,\n                 num_boost_round=20000,\n                 early_stopping_rounds=1000,\n                 stratified=False,\n                 nfold=5,\n                 verbose_eval=1000, \n                 show_stdv=False)\nelse: \n    # Fit with parameters\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=5000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\n\nAfter fitting the models, we are ready to apply the model on the factory fresh hold-out data set. Was gradient boosting able to well approximate our crazy function? To do so, we look at heatmaps of\n\n- the response y (i.e. the ground truth)\n\n- the predictions, as well as\n\n- the out-of-sample residuals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add predictions to test data\ndata_eval = pd.DataFrame(np.c_[X_test, y_test], columns=['x', 'z', 'y'])\ndata_eval[\"predictions\"] = gbm.predict(X_test)\ndata_eval[\"residuals\"] = data_eval[\"y\"] - data_eval[\"predictions\"]\n\n# Plot the results\nfig, ax = plt.subplots(1, 3, figsize=(21, 6))\nfor i, v in enumerate(['y', 'predictions', 'residuals']):\n    nice_heatmap(data_eval, v=v, ax=ax[i])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The white scatter in the image are due to plotting only the 33% hold-out sample.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Wrap up\n\nCongratulation to the gradient boosted tree! The approximation error is extremely small. Imagine how long you would have had to generate relevant features for a linear model...","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}