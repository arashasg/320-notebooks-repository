{"cells":[{"metadata":{"_uuid":"c7a91521a3b91b38debb8813b381dfc32b2eb66f"},"cell_type":"markdown","source":"# Product recommendation based on visual similarity\n\n\nThe goal of this experiment is to make a very basic recommender system: for a given fashion product, we want to recommend products that look similar. \n\nThis kind of recommender system is often used when browsing shopping websites. They usually appear on product pages as a \"you may also like\" section.\n\nThe idea behind this recommender system is simple: if a customer is showing interest towards a product by browsing its page, he may also be interested by products that are similar.\n\n\n## How to proceed ?\n\nWe will used a pre-trained CNN model from Keras to extract the image features.\n\nThen we will compute similarities between the different products using the previously extracted image features.\n\nOther type of information can be used for this purpose such as the product category, size, color, etc. if the data is available, but that is not the case here."},{"metadata":{"_uuid":"1475a1fd665a7fc6c7755c94d8e2b9042f470d54"},"cell_type":"markdown","source":"## 0. imports and parameters setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports\n\nfrom keras.applications import vgg16\nfrom keras.preprocessing.image import load_img,img_to_array\nfrom keras.models import Model\nfrom keras.applications.imagenet_utils import preprocess_input\n\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"31d39be5ff523993bb246ccafcc172c6e9aadce0"},"cell_type":"code","source":"# parameters setup\n\nimgs_path = \"../input/style/\"\nimgs_model_width, imgs_model_height = 224, 224\n\nnb_closest_images = 5 # number of most similar images to retrieve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bb6ba742ab12fa45215ed3f2bc939975432f9ef"},"cell_type":"markdown","source":"## 1. load the VGG pre-trained model from Keras\n\nKeras module contains several pre-trained models that can be loaded very easily. \n\nFor our recommender system based on visual similarity, we need to load a Convolutional Neural Network (CNN) that will be able to interpret the image contents.\n\nIn this example we will load the VGG16 model trained on imagenet, a big labeled images database.\n\nIf we take the whole model, we will get an output containing probabilities to belong to certain classes, but that is not what we want.\n\nWe want to retrieve all the information that the model was able to get in the images.\n\nIn order to do so, we have to remove the last layers of the CNN which are only used for classes predictions."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ef249050ecf032959c607bbe75fe73243de2b139","collapsed":true},"cell_type":"code","source":"# load the model\nvgg_model = vgg16.VGG16(weights='imagenet')\n\n# remove the last layers in order to get features instead of predictions\nfeat_extractor = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)\n\n# print the layers of the CNN\nfeat_extractor.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f387926367e1beb13f6f2cb400e4aa45ce521df1"},"cell_type":"markdown","source":"## 2. get the images paths"},{"metadata":{"trusted":true,"_uuid":"8a2095a63d2f1a684b945f6324eaffacf80ef623","collapsed":true},"cell_type":"code","source":"files = [imgs_path + x for x in os.listdir(imgs_path) if \"png\" in x]\n\nprint(\"number of images:\",len(files))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72fe42850da418b01a9c2835721702bf8c25df42"},"cell_type":"markdown","source":"## 3. feed one image into the CNN\n\nFirst we observe what output we get when putting one image into the CNN.\n\nThe following steps are:\n- loading the image\n- preparing the image to feed it into the CNN\n- get the CNN output which will correspond to the image features"},{"metadata":{"trusted":true,"_uuid":"e22ba7988e1d6da1cfbf48d316022eaf1c083622","collapsed":true},"cell_type":"code","source":"# load an image in PIL format\noriginal = load_img(files[0], target_size=(imgs_model_width, imgs_model_height))\nplt.imshow(original)\nplt.show()\nprint(\"image loaded successfully!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0402651e962e116504cd3a60a2bd891ec36cc04b","collapsed":true},"cell_type":"code","source":"# convert the PIL image to a numpy array\n# in PIL - image is in (width, height, channel)\n# in Numpy - image is in (height, width, channel)\nnumpy_image = img_to_array(original)\n\n# convert the image / images into batch format\n# expand_dims will add an extra dimension to the data at a particular axis\n# we want the input matrix to the network to be of the form (batchsize, height, width, channels)\n# thus we add the extra dimension to the axis 0.\nimage_batch = np.expand_dims(numpy_image, axis=0)\nprint('image batch size', image_batch.shape)\n\n# prepare the image for the VGG model\nprocessed_image = preprocess_input(image_batch.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef55943fb35a862592d82c4e34f2f0b8c0b1cc07","collapsed":true},"cell_type":"code","source":"# get the extracted features\nimg_features = feat_extractor.predict(processed_image)\n\nprint(\"features successfully extracted!\")\nprint(\"number of image features:\",img_features.size)\nimg_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6f75b92124f354f042ef55c64d0262b3d5dbd6e"},"cell_type":"markdown","source":"## 4. feed all the images into the CNN\n\nWe were able to do the feature extraction process for one image. Now let's do it for all our images!"},{"metadata":{"trusted":true,"_uuid":"ce00de828b3c069d0f428b3acb2c9c97e6e0abe9","collapsed":true},"cell_type":"code","source":"# load all the images and prepare them for feeding into the CNN\n\nimportedImages = []\n\nfor f in files:\n    filename = f\n    original = load_img(filename, target_size=(224, 224))\n    numpy_image = img_to_array(original)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    importedImages.append(image_batch)\n    \nimages = np.vstack(importedImages)\n\nprocessed_imgs = preprocess_input(images.copy())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"213926aeb8cb456cc2f733f8ee5b4c9b6f09fc99","collapsed":true},"cell_type":"code","source":"# extract the images features\n\nimgs_features = feat_extractor.predict(processed_imgs)\n\nprint(\"features successfully extracted!\")\nimgs_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a53b5b1daede0913380768c3867a442ae4e923"},"cell_type":"markdown","source":"# 5. compute cosine similarities\n\nNow that we have features for every image, we can compute similarity metrics between every image couple.\n\nWe will use here the cosine similarity metric."},{"metadata":{"trusted":true,"_uuid":"0c7655e45c67132cbdcf899a4aaad6741bccb16b","scrolled":true,"collapsed":true},"cell_type":"code","source":"# compute cosine similarities between images\n\ncosSimilarities = cosine_similarity(imgs_features)\n\n# store the results into a pandas dataframe\n\ncos_similarities_df = pd.DataFrame(cosSimilarities, columns=files, index=files)\ncos_similarities_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8bafad09dc678ae301194bf7c591e2a4ce7056c4"},"cell_type":"markdown","source":"# 6. retrieve most similar products\n\nThe final step is to implement a function that, for any given product, returns the visually most similar products."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2ffb452f95bc65eed48d60bfabbc06360ab8504f"},"cell_type":"code","source":"# function to retrieve the most similar products for a given one\n\ndef retrieve_most_similar_products(given_img):\n\n    print(\"-----------------------------------------------------------------------\")\n    print(\"original product:\")\n\n    original = load_img(given_img, target_size=(imgs_model_width, imgs_model_height))\n    plt.imshow(original)\n    plt.show()\n\n    print(\"-----------------------------------------------------------------------\")\n    print(\"most similar products:\")\n\n    closest_imgs = cos_similarities_df[given_img].sort_values(ascending=False)[1:nb_closest_images+1].index\n    closest_imgs_scores = cos_similarities_df[given_img].sort_values(ascending=False)[1:nb_closest_images+1]\n\n    for i in range(0,len(closest_imgs)):\n        original = load_img(closest_imgs[i], target_size=(imgs_model_width, imgs_model_height))\n        plt.imshow(original)\n        plt.show()\n        print(\"similarity score : \",closest_imgs_scores[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa813811e21efc0a550495840658cd7f2e09e66c","collapsed":true},"cell_type":"code","source":"retrieve_most_similar_products(files[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b12c4d58961bf426a51daaf5b343a2bce9b52eb8","collapsed":true},"cell_type":"code","source":"retrieve_most_similar_products(files[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24c3b0d7710b505e398940fc0b7fe515789ada41","collapsed":true},"cell_type":"code","source":"retrieve_most_similar_products(files[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1b2bccae32b1dd4969b785def390ce54933da28","collapsed":true},"cell_type":"code","source":"retrieve_most_similar_products(files[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47c0ab3b43a5622c7832f6f506f4a570f16f5c14","collapsed":true},"cell_type":"code","source":"retrieve_most_similar_products(files[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"867e7d1281edd9451a56634771c699b9d7c9dfbb"},"cell_type":"markdown","source":"# Conclusion\nWe saw above that this very basic recommender system is able to find similar products accurately: most of the time the retrieved products have the same purpose and even look very similar.\n\nThis could be incorporated directly into a website using a web framework such as Flask."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}