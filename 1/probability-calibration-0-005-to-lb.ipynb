{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"In iMaterialist Furniture Challenge we have classical problem when class distribution is different for train and test.\n\nRecently on Kaggle in Quora Question Pairs we had the same problem and you can read about it here https://www.kaggle.com/c/quora-question-pairs/discussion/31179 . But I'll try to explain solution in slightly different way and show how to apply it for multiclass problem."},{"metadata":{"_uuid":"6f7966a4eeacc18662826d366982bc48396106a4"},"cell_type":"markdown","source":"In this competition 128 classes and they distributed like this for train and validation:"},{"metadata":{"trusted":true,"_uuid":"89da58fde16375b74690eab6e1ba57eed768cce7","collapsed":true},"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-dark')\n\ntrain_json = json.load(open('../input/imaterialist-challenge-furniture-2018/train.json'))\ntrain_df = pd.DataFrame(train_json['annotations'])\n\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\nax0.hist(train_df.label_id.value_counts())\nax0.set_xlabel(\"# images per class\")\nax0.set_ylabel(\"# classes\")\nax0.set_title('Class distribution for Train')\n\nval_pred = np.loadtxt('../input/furniture2018val/furniture_val_true.csv')\nax1.hist(pd.Series(val_pred).value_counts())\nax1.set_xlabel(\"# images per class\")\nax1.set_title('Class distribution for Validation')\nf;","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"dbd1df6c2adac07f18d768c1c208abbc9df80a5c"},"cell_type":"markdown","source":"In train some classes have 4k examples other only 500. It's 4x difference. But validation has balanced dataset - 50 images per class. We also can assume from number of images in test (12800=128*100) - it's balanced too."},{"metadata":{"_uuid":"202b65faa157e5f7256899fe65b5a0e8e115861d"},"cell_type":"markdown","source":"I've tried oversampling and using weigths for CrossEntropy loss to solve the problem. But the best score I got with calibration."},{"metadata":{"_uuid":"fa7739f049dd4704a5a2bf3da7d02e723893e9d7"},"cell_type":"markdown","source":"## Probability calibration"},{"metadata":{"_uuid":"06240db5f439e53d6e1695e058a65f598b92c46a"},"cell_type":"markdown","source":"Let's start with 2 class problem. From Bayesian perspective our final predicted probability from unbalanced train dataset could be seen as:\n\n$$\n(0)\n\\left\\{\n\\begin{array}{rl}\n P(y_0|X) \\propto Pr(y_0) L(X|y_0)  \\\\\n P(y_1|X) \\propto Pr(y_1) L(X|y_1)\n\\end{array}\n\\right. \n$$\n\n$P(y_0|X)$ - predicted probability for class $y_0$; $Pr(y_0)$ - prior probability for $y_0$; $L(X|y_0)$ some likelihood of some data for $y_0$. I'm using $L$ and $Pr$ to not confuse you with bunch of $P$.\n\nIn words, our predicted probability is multiplication of some prior probability and some function from X.\n\nFor different distribution, likelihood should be the same, but because prior is different, we get different probability:\n\n$$\n(1)\n\\left\\{\n\\begin{array}{rl}\n P(y_0|X)' \\propto Pr(y_0)' L(X|y_0)  \\\\\n P(y_1|X)' \\propto Pr(y_1)' L(X|y_1)\n\\end{array}\n\\right.\n$$\n\n$P(y_0|X)'$ is our corrected probability which we need to calculate. And we know desire priors (1/2 for balanced), and becase likelihood is the same we can get it from (0):\n\n$$\n(2)\n\\left\\{\n\\begin{array}{rl}\n L(X|y_0)  \\propto \\frac{P(y_0|X)}{Pr(y_0)}   \\\\\n L(X|y_1)  \\propto \\frac{P(y_1|X)}{Pr(y_1)}\n\\end{array}\n\\right. \n$$\n\nand insert likelihood in (1) to get corrected probability:\n$$\n(3)\n\\left\\{\n\\begin{array}{rl}\n P(y_0|X)' \\propto Pr(y_0)' \\frac{P(y_0|X)}{Pr(y_0)})  \\\\\n P(y_1|X)' \\propto Pr(y_1)' \\frac{P(y_1|X)}{Pr(y_1)}\n\\end{array}\n\\right.\n$$\nand it's almost our final formula, we just need to normalize probability so they sum to 1."},{"metadata":{"_uuid":"8bd7e73e084ff4c98fb2004d23c52bb60e526bad"},"cell_type":"markdown","source":"Changing this formula to multiclass is quite easy. For every class we just represent other 127 classes as one class."},{"metadata":{"_uuid":"5a8e781156dd1de1cf9ababf141f87c7d0f304ae"},"cell_type":"markdown","source":"We're ready to move to practice.\n\nWhere I'll also use:\n$$\nP(y_1|X) = 1 - P(y_0|X) \\\\\nPr(y_1) = 1 - Pr(y_0) \\\\\nPr(y_1)' = 1 - Pr(y_0)' \\\\\nPr(y_0)' = \\frac{1}{128} \\\\\n$$"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"195b203705111d16e5f76705a16b50122f0da4d8"},"cell_type":"code","source":"def calibrate(prior_y0_train, prior_y0_test,\n              prior_y1_train, prior_y1_test,\n              predicted_prob_y0):\n    predicted_prob_y1 = (1 - predicted_prob_y0)\n    \n    p_y0 = prior_y0_test * (predicted_prob_y0 / prior_y0_train)\n    p_y1 = prior_y1_test * (predicted_prob_y1 / prior_y1_train)\n    return p_y0 / (p_y0 + p_y1)  # normalization","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"01da3ba064da8501279b2d17bb631767df5bcdd3"},"cell_type":"code","source":"prior_y0_test = 1/128\nprior_y1_test = 1 - prior_y0_test\n\ndef calibrate_probs(prob):\n    calibrated_prob = np.zeros_like(prob)\n    nb_train = train_df.shape[0]\n    for class_ in range(128): # enumerate all classes\n        prior_y0_train = ((train_df.label_id - 1) == class_).mean()\n        prior_y1_train = 1 - prior_y0_train\n        \n        for i in range(prob.shape[0]): # enumerate every probability for a class\n            predicted_prob_y0 = prob[i, class_]\n            calibrated_prob_y0 = calibrate(\n                prior_y0_train, prior_y0_test,\n                prior_y1_train, prior_y1_test,                \n                predicted_prob_y0)\n            calibrated_prob[i, class_] = calibrated_prob_y0\n    return calibrated_prob","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b17c58e9cc0eedb4a3f0b877362a1f30fc5ed7f8","collapsed":true},"cell_type":"code","source":"# let's apply calibration to validation \nval_prob = np.loadtxt('../input/furniture2018val/furniture_val_prob.csv', delimiter=',')\ncalibrated_val_prob = calibrate_probs(val_prob)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe19032548442a45859063a01245513d589be369","collapsed":true},"cell_type":"code","source":"val_predicted = np.argmax(val_prob, axis=1)\ncalibrated_val_predicted = np.argmax(calibrated_val_prob, axis=1)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0088022a9f38d741451fe7c48b4f0961fb296796","collapsed":true},"cell_type":"code","source":"f, (ax0, ax1, ax2) = plt.subplots(3, 1, sharex=True, figsize=(10, 10))\nax0.hist(pd.Series(val_predicted).value_counts(), bins=20)\nax0.set_xlabel('# images per class')\nax0.set_ylabel(\"# classes\")\nax0.set_title('Before')\n\nax1.hist(pd.Series(calibrated_val_predicted).value_counts(), bins=20)\nax1.set_xlabel('# images per class')\nax0.set_ylabel(\"# classes\")\nax1.set_title('After')\n\nax2.hist(pd.Series(list(range(128))*50).value_counts())\nax2.set_xlabel('# images per class')\nax0.set_ylabel(\"# classes\")\nax2.set_title('Ideal')\nf;","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"c19e6cb331fcbae5c70116807f51e5749acc9e63"},"cell_type":"markdown","source":"So our calibrated probability is slightly close to our ideal distribution.\n\nLet's see how good calibration for score:"},{"metadata":{"trusted":true,"_uuid":"f48546a0b2b04b4914abbb121f36976c5d911033","collapsed":true},"cell_type":"code","source":"val_true = np.loadtxt('../input/furniture2018val/furniture_val_true.csv', delimiter=',')\nprint('Score for raw probability:', (val_true != val_predicted).mean())\nprint('Score for calibrated probability:', (val_true != calibrated_val_predicted).mean())","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"e48bdc251adfc02f6bd28ff089022b45449fcf2b"},"cell_type":"markdown","source":"So we got 0.005 improvement."},{"metadata":{"_uuid":"da75d6d7df68575c72bde0b6f036391770f6aaab"},"cell_type":"markdown","source":"Happy Kaggling!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}