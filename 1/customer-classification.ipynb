{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Santander Customer Satisfaction**\n\n**Problem statement:-**\n\nFrom frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.\n\nThis kernel will help Santander Bank to identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\n\nWe are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals 1 for unsatisfied customers and 0 for satisfied customers.\n\nThe task is to predict the probability that each customer in the test set is an unsatisfied customer.\n\n![Customer Satisfaction](https://cdn.dribbble.com/users/489445/screenshots/1819359/pyxl_blog_creating_customer_experience_2.gif)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Import important libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport random\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets load"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\ntest = pd.read_csv('../input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We saw there are only numerical columns in this dataset. However looking at the range it seems there are many outliers here"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values present either\n\nLets check one of the column to confirm our theory about outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['imp_ent_var16_ult1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes there it is . We can clearly see there is only 1 value of 17595.15 which definitely serve as an outlier. To clean this dataset outlier ommission is needed."},{"metadata":{},"cell_type":"markdown","source":"# Outlier handling"},{"metadata":{},"cell_type":"markdown","source":"First we will see the values of different quartiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\ntrain.describe(percentiles=[.25,.5,.75,.90,.95, .975,.99,.999])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By analysing this visually it seems that ranges 0.99, 0.25, 0.75 seems important. Lets save their value somewhere"},{"metadata":{"trusted":true},"cell_type":"code","source":"high = .99\nfirst_quartile = 0.25\nthird_quartile = 0.75\nquant_df = train.quantile([high, first_quartile, third_quartile])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have our quartile dataframe"},{"metadata":{},"cell_type":"markdown","source":"Lets now prepare and clean our training dataset.\n\nFirst we need to remove TARGET and ID column from our training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.drop(['ID', 'TARGET'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets take 99% as threshold for outlier. Drop all values above 0.99 percentile"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.apply(lambda x: x[(x <= quant_df.loc[high,x.name])], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we didnt have that much data loss and also our data is now free of outliers.\n\nNow lets get back our ID and TARGET columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train.loc[:,'ID'], train_df], axis=1)\n\ntrain_df = pd.concat([train.loc[:,'TARGET'], train_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dropping outliers we can see that we have now encountered some null values in our dataset. We need to cater this issue.\n"},{"metadata":{},"cell_type":"markdown","source":"# Handling null values"},{"metadata":{},"cell_type":"markdown","source":"To handle null values we will take a random value between minimum and maximum value of each column and use that random value for imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_df = train_df\nfor col in new_train_df.columns:\n    min_val = min(new_train_df[col])\n    max_val = max(new_train_df[col])\n    new_train_df[col].fillna(round(random.uniform(min_val, max_val), 2), inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_df.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see our dataset is free of null values :)"},{"metadata":{},"cell_type":"markdown","source":"Now lets go to model testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = new_train_df['TARGET']\nX = new_train_df.drop(['TARGET','ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The much needed train test split for cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy with Logistic is quite good. But hold on do we have a balanced dataset. Maybe not. Lets check if imbalance is the root cause of this high accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_df['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that almost 96% of dataset is having value 0. We need to balance this dataset to get better results"},{"metadata":{},"cell_type":"markdown","source":"### Dataset balancing\nWe will use **SMOTE** technique to balance the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_tr.shape)\nprint(y_tr.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets now fit the data"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_tr,y_tr)\n\nlr_preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, lr_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_tr, y_tr)\n\ndt_preds = dt1.predict(X_test)\nprint(\"Accuracy with Decision Tree = \", metrics.accuracy_score(y_test, dt_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rft = RandomForestClassifier(n_jobs=-1)\nrft.fit(X_tr, y_tr)\n\nrft_preds = rft.predict(X_test)\nprint(\"Accuracy with Random Forest = \", metrics.accuracy_score(y_test, rft_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have got multiple models with all their accuracies. We can choose anyone of them to make further predictions."},{"metadata":{},"cell_type":"markdown","source":"As RFT has maximum accuracy lets use that."},{"metadata":{},"cell_type":"markdown","source":"# Final Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_final = test.drop(['ID'], axis=1)\nfinal_prediction = rft.predict(x_test_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"ID\": test[\"ID\"],\n        \"TARGET\": final_prediction\n    })\nsubmission.to_csv('RandomForect.csv',header=True, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}