{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Preprocessing\n2. Data exploration\n3. Choose right number of clusters\n4. Present business interpretation of clusters"},{"metadata":{},"cell_type":"markdown","source":"### 1. Preprocessing\n#### Include necessary libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom mpl_toolkits.axes_grid1 import host_subplot\nfrom sklearn import metrics\nimport matplotlib.gridspec as gridspec\nimport math\nimport seaborn as sns\n\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load data "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv'\ndata = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data exploration\n#### Data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in data.columns:\n    print(column, ' - ', data[column].dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing valeus\n It's important to deal with missing values. If there're NA's, it's important to replace or delete rows. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    nan = round(data[col].isnull().sum() / len(data[col]) * 100, 2)\n    print(col, ' : ', nan, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram's for numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation plot for numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,vmin = -1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap presents no significant correlation between any of features."},{"metadata":{},"cell_type":"markdown","source":"#### Stanrarize data\nIf features are measured on diffrent scales it's good to standarize data. Diffrent scale may cause that one point is close to another in one direction but very far in other. Data standarization prevent this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standarize data\nscaler = StandardScaler()\ndata_sc = scaler.fit_transform(data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])\ndata_sc = pd.DataFrame(data_sc, columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Choose right number of clusters\nElbow plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def elbow_plot_for_k_means(X, k_range = range(2,15)):\n    distortions = []\n    for k in k_range:\n        kmeanModel = KMeans(n_clusters=k).fit(X)\n        kmeanModel.fit(X)\n        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0]) \n    reduction = [distortions[i-1] - distortions[i] for i in range(1,len(distortions))]\n    reduction.insert(0,np.nan)\n    # Plot    \n    plt.figure(figsize=[12,8])\n    host = host_subplot(111)\n    par = host.twinx()\n    host.set_title(\"Optimal number of clusters\")\n    host.set_xlabel(\"k\")\n    host.set_ylabel(\"Distortion\")\n    par.set_ylabel(\"Distortion reduction\")\n    plt.xticks(k_range)\n    p1, = host.plot(k_range, distortions)\n    p2, = par.plot(k_range, reduction)\n    host.yaxis.get_label().set_color(p1.get_color())\n    par.yaxis.get_label().set_color(p2.get_color())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elbow_plot_for_k_means(data_sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calinski - Harabasz plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ch_plot_for_k_means(X, k_range = range(2,15), resample = 3):\n    plt.figure(figsize=[12,8])\n    for i in range(3):\n        scores = []\n        for k in k_range:\n            kmeansModel = KMeans(n_clusters = k).fit(X)\n            labels = kmeansModel.labels_\n            scores.append(metrics.calinski_harabasz_score(X, labels)) \n        plt.plot(k_range, scores)\n    plt.xticks(k_range)\n    plt.title(\"Optimal number of clusters Calinski-Harabasz criterion\")\n    plt.xlabel(\"k\")\n    plt.ylabel(\"Calinski - Harabasz statistic\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ch_plot_for_k_means(data_sc, k_range = range(2,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CH plot shows that we can expect 6 optimal clusters. Elbow plot is less clear, but line stabilizes after 6 clusters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class KMeansCust(KMeans):\n    \"\"\"\n    Custumized KMeans Class. Add new prediction method, returned cluster label and distances to each cluster. \n    It's usefull when using distances as explanatory variables in other model.\n    \"\"\"    \n    def predict_dist(self, X, columns = None, scaler = True, join = True):\n        \"\"\"\n        There arent missing values in X. \n        IN: X - data (pandas, numpy)\n            columns - if X is pandas obiect then its possible to indicate columns and X can have redundant columns.\n            scaler - if True then StandardScaler\n            inplace - if True results are joined to X and then \n        \"\"\"\n        if columns != None: data = X[columns]\n        else: data = X\n        if scaler:\n            scaler = StandardScaler()\n            data = scaler.fit_transform(data)\n        df = pd.DataFrame(np.array(cdist(data, self.cluster_centers_, 'euclidean')))\n        df.columns = ['dist_clu_' + str(i) for i in range(len(df.columns))]\n        labels = self.predict(data)\n        df['clu_label'] = list(labels)\n        if join:\n            X_ = X.copy()\n            X_.reset_index(inplace = True, drop = True)\n            df.reset_index(inplace = True, drop = True)\n            return X_.join(df, how = 'left')\n        else:\n            return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fit model"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_model = KMeansCust(n_clusters = 6).fit(data_sc)\ndata_pred = kmeans_model.predict_dist(X = data, columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Present business interpretation of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kmeans_present_cluster_stats(data, label, cols = None):\n    \"\"\"\n    \"\"\"\n    means = data.groupby(label).mean()\n    rows = math.ceil(len(means.columns)/5)\n    fig = plt.figure(constrained_layout=False, figsize=(10, rows * 5))\n    gs = gridspec.GridSpec(rows, 5, figure=fig)\n    i=0\n    for col in means.columns:\n        if col in cols:\n            ax = fig.add_subplot(gs[i])\n            if i % 5 == 0:\n                ax.set_yticks([i + 0.5 for i in means.index])\n                ax.set_yticklabels(['Cluster ' + str(i) for i in means.index])\n                ax.set_ylabel('Mean by cluster')\n            else:\n                ax.set_yticks([])\n            ax.set_xticks([0.5])\n            ax.set_xticklabels([col], rotation = 45)\n            col_t = [[i] for i in list(means[col])] \n            im = ax.pcolormesh(col_t, cmap=\"RdYlBu_r\")\n            plt.colorbar(im)\n            i+=1\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_present_cluster_stats(data = data_pred, label = 'clu_label', cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation: <br>\nCluster 0 - medium age, high income, economical <br>\nCluster 1 - medium age, high income, high spending <br>\nCluster 2 - old, avarage income, avarage spending <br>\nCluster 3 - young, avarage income, avarage spending <br>\nCluster 4 - young, low income, high spending - intresting :) <br> \nCluster 5 - medium age, low income, low spending"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}